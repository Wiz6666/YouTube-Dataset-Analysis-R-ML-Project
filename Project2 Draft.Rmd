---
title: "CITS4009-project2"
author: "Wiz ZHANG(SID:23210735)； Xiao ZHANG"

output:   
  html_document:
    highlight: haddock
    theme: cosmo
    code_folding: hide
---
```{r setup, include=FALSE}
#Set all code chunks to display their R code by default
knitr::opts_chunk$set(echo = TRUE)
```








# Part1 - Introduction




## 1.1 - Project Background and Purpose
  In this project, our objective is to illustrate the process of modeling by using R machine learning functions. We aim to understand and apply various stages of data science including data preparation, model building, and model evaluation. We have choosen the YouTube dataset which is shared by uni coordinator. We also use the same dataset in our Project 1. The specific contributions of each member will be separated 50-50 fairly.
  
  
  
  
## 1.2 Data Source and Characteristics
  The data set analyzed can be obtained from Kaggle platform. It comes from the "Global YouTube Statistics 2023". (https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023)
  A collection of YouTube giants, this dataset offers a perfect avenue to analyze and gain valuable insights from the luminaries of the platform. With comprehensive details on top creators' subscriber counts, video views, upload frequency, country of origin, earnings, and more, this treasure trove of information is a must-explore for aspiring content creators, data enthusiasts, and anyone intrigued by the ever-evolving online content landscape. Immerse yourself in the world of YouTube success and unlock a wealth of knowledge with this extraordinary dataset.[@Elgiriyewithana2023]








# Part2 - Setup





## 2.1 Load required R packages
  We load 'ggplot2' package [@Wickham2016] for making static graphics and 'dplyr' package [@Wickham2021] for data manipulation.
  We employ scatterplot matrices and correlation heatmaps for exploratory data analysis to understand the pairwise relationships between variables [@Friendly2002]...
  We use the skimr package to obtain more comprehensive descriptive statistics of the numeric variables in the dataset [@McNamara2021]...
```{r Library, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggthemes)
library(numform)
library(treemapify)
library(timeDate)
library(lubridate)
library(reshape2)
library(ca)
library(skimr)
library(janitor)
library(flextable)
library(shiny)
library(leaflet)
library(maps)
library(RColorBrewer)
library(scales)
library(readr)
library(forecast)
library(caret)

```




## 2.2 Set plot theme 
  The theme is set up to enhance the readability and interpretability of the graphs, following the best practices in data visualization [@Healy2018].
```{r Theme}
project_theme <- theme(
  panel.background = element_rect(fill = "#FFFBDC"),  # Light yellow background
  panel.grid.major = element_line(color = "#FFE4A1"), # Light orange major grid lines
  panel.grid.minor = element_blank(), # Remove minor grid lines
  plot.title = element_text(size = 18, hjust = 0.5, color = "darkblue"),  # Title color and size
  axis.title = element_text(size = 16, color = "darkblue"),  # Axis title color and size
  axis.text = element_text(size = 14, color = "black"),   # Axis text color and size
  legend.title = element_text(size = 16, color = "darkblue"), # Legend title color and size
  legend.text = element_text(size = 14, color = "black"),   # Legend text color and size
  legend.background = element_rect(fill = "#FFFBDC"),  # Legend background color
  plot.background = element_rect(fill = "#FFFBDC")   # Background color of the entire plot
)
```




## 2.3 Load the main data
  In this part, we will load the main dataset, store the path in 'Data_path'
```{r readfile, warning=FALSE}
data_path <- 'Global YouTube Statistics.csv'
raw_data <- read.csv(data_path,encoding = "UTF-8")
```




## 2.4 Dataset initial overview
  Before starting in-depth analysis and modeling, it is crucial to perform an initial overview of the dataset. By using functions such as summary, str, etc., we can gain an overall understanding of the data, including data types, data distribution, missing values and possible outliers.
    This is the process that we already done in Project 1, according to Professor Wei, we don't need to do it again so we pass here. So we shorten the content about dataset overview. Mainly is using str; summary;head command to check dataset overview, we also use a function to check dataset skewness. We also shorten or even skip the EDA part in Project 2 report.
  First, we use str command to analyze the data
```{r str,results='hide', echo=FALSE, warning=FALSE}
str(raw_data)
```

The "Global YouTube Statistics" dataset contains 995 observations and 28 variables.
  Then we would use summary command to analyze data. 
  we use skimr here, which is similar to summary command but it shows more descriptive statistics for numeric variables than summary command.
```{R Skimr Summary,results='hide', echo=TRUE, warning=FALSE}
skimr::skim(raw_data) |>
  flextable::flextable() 
```

  We can see from the standard output that 'video_views_rank' has 1 missing value, country_rank has 116, 'subscribers_for_last_30_days' has 337 the most in the whole dataset.
  The numbers of missing value of 'Gross.tertiary.education.enrollment.rate','Population', 'Unemployment.rate', 'Urban_population', 'Latitude' are the same , so it should be a systematic missing issue. So we need to pay attention to this issue in the following data cleaning.
  According to the numeric.hist, it seems most of the numeric variables are right skewnessed.The 'Unemployment.rate' does not look like a normal distribution,so as 'Longitude'.
  Skewness analysis is crucial as it helps in understanding the symmetry of the data distribution [@JoanesGill1998]. We observe a higher percentage of right-skewed columns in our dataset, which implies that most numerical variables in our dataset have a distribution that has a tail on the right side, meaning the right side of the distribution is longer or fatter than the left side. This can impact the analysis and model performance because many statistical methods assume a normal distribution of the data. Right-skewed distributions are associated with higher values and outliers on the right side of the distribution which can greatly impact the mean and variance, and subsequently, distort the interpretation and performance of the model.
  Addressing skewness is often important in regression models where the assumption of normally distributed residuals is made. Therefore, recognizing the presence of skewness can help us make informed decisions about data transformation techniques, such as logarithmic transformation or Box-Cox transformation, to attempt to normalize the data distribution before applying any modeling techniques. For example, this can be particularly relevant for variables like `subscribers`, `uploads`, and `lowest_monthly_earnings`, where skewness might impact the model’s ability to generalize well to unseen data and might lead to unreliable predictions or insights.
  
  Calculate Right skewness numbers and left skewness numbers
```{r Skewness Calculator}
calculate_skewness_percentage <- function(dataframe) {
  #initialize function variables
  right_skewed_count <- 0
  left_skewed_count <- 0
  column_names <- names(dataframe)
  #count the numbers with loops
  for (column in column_names) {
    if (is.numeric(dataframe[[column]])) {
      median_value <- median(dataframe[[column]], na.rm = TRUE)
      mean_value <- mean(dataframe[[column]], na.rm = TRUE)
      if (mean_value > median_value) {
        right_skewed_count <- right_skewed_count +1
      } else if (mean_value < median_value) {
        left_skewed_count <- left_skewed_count +1
      }
    }
  }
  #calculate percentage with the count result
  right_percentage <- round((right_skewed_count / ncol(dataframe)) * 100, 2)
  left_percentage <- round((left_skewed_count / ncol(dataframe)) * 100, 2) 
  
  return(list(right_percentage, left_percentage))  
}

# Use function to get the calculation result
results <- calculate_skewness_percentage(raw_data)
cat("\n")
cat("Percentage of right-skewed columns is:", results[[1]], "%\n")
cat("Percentage of left-skewed columns is:", results[[2]], "%\n")
rm(results)
```
  The outcomes of our skewness analysis are telling and significant, revealing that 50% of the columns in our dataset are right-skewed, in contrast to a mere 21.43% being left-skewed. This preponderance of right-skewed columns is pivotal as it unveils a tendency for the distribution of most numerical variables to be tail-heavy on the right side, a characteristic that might have consequential implications for subsequent analyses and modeling efforts.
  The presence of such skewed columns underlines the existence of numerous higher values and potential outliers on the right side of the distribution. These extreme values have the capability to significantly inflate the mean and variance of the data, potentially leading to distorted interpretations and undermining the reliability and robustness of our subsequent statistical models.
  The substantial right skewness observed in specific variables such as subscribers, uploads, and lowest_monthly_earnings could particularly impact the predictive power and generalizability of the models. Such skewness could render the models less resilient and adaptive to unseen or new data, thereby possibly yielding unreliable predictions or insights.
  Employing such transformations could help mitigate the impact of skewness and align the data more closely with the assumptions of many statistical methods, predominantly enhancing the reliability and validity of our subsequent analyses and models.

  Then we use head command to keep analyzing data.
```{r head, results='hide', echo=FALSE, warning=FALSE}
head(raw_data,n=100)
```
  Base on the standard output, we can see that subscribers shows how many people subscribe the Youtuber. The Title is the same as Youtuber, we need to figure out why it is the same but in two different columns. the Music Youtuber has 119000000 subsribers but only 0 uploads , so we need to make data cleaning later.








# Part3 - Data Cleaning and Preparation




## 3.1 Data Cleaning


### 3.1.1 Check column names and make adjustments

  First, we use names() command to check each variable name.
```{r names,results='hide'}
names(raw_data)
```

  Then , we use janitor package and rename function adjust variable names to ensure uniform formatting.
  
  janitor::clean_names() is a function in the package whose main purpose is to convert the column names of dataframes into a clean, uniform format. Here is the main logic and rules of this function:
1.All characters to lowercase
2.Spaces are converted to underscores
3.Non-alphanumeric characters are removed
4.Numbers before text moved to the end
5.Underline after keyword in R)
```{r Name Changer}
raw_data <- janitor::clean_names(raw_data)
```

  From the output result we can see most of the variable names have been adjusted properly. But some of them still need manual adjustment for better understanding and clearness.
```{r Name Changer2}
raw_data <- raw_data %>%
  rename(
    #More descriptive names
    video_views_last_30_days = video_views_for_the_last_30_days,
    country_abbr = abbreviation,
    #Shorter names
    monthly_earnings_low = lowest_monthly_earnings,
    monthly_earning_high = highest_monthly_earnings,
    yearly_earning_low = lowest_yearly_earnings,
    yearly_earning_high = highest_yearly_earnings,
    subs_last_30_days = subscribers_for_last_30_days,
    tertiary_edu_enrollment = gross_tertiary_education_enrollment,
    )
print(colnames(raw_data))
```
  
  
### 3.1.2 Inspect & Adjust data types
Use sapply function to check data types, adjust wrong data typs.
```{r Data type, results='hide', echo=FALSE,warning=FALSE}
data_types <- sapply(raw_data,class)
data_sample <- sapply(raw_data,head)
print(data_types)
print(data_sample)
rm(data_types, data_sample)
```

  From the standard output first, we know that we need to handle the missing values; 'created_month' should change from char type to numeric type this is useful for following analysis; 'category','country','channel_type','abbreviation' shoule be factor type which is better for following analysis;handle the outliers according to the skimmr result.
  
  
### 3.1.3 Handle Missing value
  We've identified three representations for missing data in our dataset: "0", "NaN", and "nan". Our goal is to standardize these by converting them to NA. Following this conversion, we aim to tally the number of NA values in each column. It's our assumption that "0" serves as a sentinel value indicating missing data. 
```{r Kill 0&NA and delete NA}
#Change 0 or 'nan' or 'NaN' to NA value
#Char columns
raw_data[raw_data == 'nan' | raw_data == "NaN" | raw_data == 0] <- NA
#Numeric columns
raw_data <- raw_data |>
  mutate(across(where(is.numeric), ~ifelse(is.nan(.), NA, .)))
```

  Use function to calculate how many missing values in each column, handle them.
```{r col_NA calculator}
# Calculate missing value number
count_missing <- function(df) {
  sapply(df, FUN = function(col) sum(is.na(col)) )
}
# Calculate percentage of missing value
percentage_missing <- function(df) {
  sapply(df, FUN = function(col) round(sum(is.na(col)) / length(col) * 100, 2) )
}

nacounts <- count_missing(raw_data)
napercents <- percentage_missing(raw_data)

# output the result
hasNA = which(nacounts > 0)
data.frame(Column = names(nacounts[hasNA]), 
           Missing_Values = nacounts[hasNA], 
           Percentage_Missing = napercents[hasNA])
rm(col,hasNA,nacounts,napercents)

```
  Our dataset examination reveals pervasive missing data across multiple columns. Given that our dataset comprises only 995 rows, a sweeping removal of missing values could severely curtail the dataset's utility for subsequent modeling. In light of this, we've devised tailored strategies for addressing different variable types.

Before delving into the strategies for managing missing values, it's crucial to outline which variables we have decided not to process and provide the rationale behind these choices:
  1.Abbreviation: Although it's feasible to replace its missing entries with 'Unknown', a total of 122 missing entries renders it impractical for further processing.
  2.Latitude & Longitude: These columns lack intrinsic geographical data. With missing data exceeding 10% of the total, they are deemed not fit for subsequent modeling tasks.
  3.Rank, Subscribers, Title & Youtuber: These columns will not be part of our processing pipeline since they manifest no missing values.

Now, let's delve into the specifics of our chosen strategies for different columns:
  1.Video Views: With a scant presence of missing values, we grappled with options between row deletion or median imputation. Given our preference to retain as many rows as possible and the data's skewed distribution, we've opted for median imputation.
  2.Country & Abbreviation: The magnitude of missing values (122 in total) poses challenges. Hence, rather than impute, we'll earmark these columns for exclusion from subsequent analyses.
  3.Earnings: Given its skewed distribution, our chosen strategy leans towards median imputation.
  4.Latitude & Longitude: These will be excluded from further analysis due to substantial missing values and an absence of auxiliary geographical information to assist imputation.
  5.Time Variables: These aren't continuous time series, ruling out techniques like ARIMA modeling. Given the paucity of missing data (only 5 rows), which equates to a mere 0.5% of the dataset, we've resolved to delete rows with missing time entries, without attempting imputation.
          
The numeric columns slated for processing are as follows:
  c("video_views", "uploads", "video_views_rank", "country_rank", "channel_type_rank", "video_views_last_30_days", "monthly_earnings_low", "monthly_earning_high", "yearly_earning_low", "yearly_earning_high", "subs_last_30_days", "tertiary_edu_enrollment", "population", "unemployment_rate", "urban_population").

The categorical columns in our crosshairs are:
  c("category", "country", "country_abbr", "channel_type").

  Our immediate focus will be on the aforementioned numeric columns. Leaning on the outcomes of our skewness analysis, which indicated a predominantly right-skewed data distribution, we've favored median imputation over mean imputation. This is predicated on the fact that the median, unlike the mean, won't perturb the right-skewed nature of the data.

In summary, our overarching strategy centers on the preservation of data integrity, and our chosen imputation methods are reflective of each column's inherent properties and data distribution.
```{r Handle missing value (numeric cols)}
change_to_median_cols <- c("video_views",
"uploads","video_views_rank","country_rank","channel_type_rank","video_views_last_30_days","monthly_earnings_low",	"monthly_earning_high","yearly_earning_low",	 "yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate","urban_population")
for (col in change_to_median_cols) {
    median_val <- median(raw_data[[col]], na.rm = TRUE)
    raw_data[[col]][is.na(raw_data[[col]])] <- median_val
}
rm(col,median_val,change_to_median_cols)
```

  Change the NAs in character variables into "Unknown"
```{r Handle missing value (character cols)}
character_columns <- c("category", "country","country_abbr","channel_type")
  
for (col in character_columns) {
  raw_data[[col]][is.na(raw_data[[col]])] <- "Unknown"
}
rm(col,character_columns)
```

  Delete all the missing rows (5 rows) in the time column. Since there are only 5 rows, it does not affect the subsequent analysis. At the same time, delete the rows with the channel creation year less than 2005, because the YouTube company was created in 2005.
```{r Delete rows}
raw_data <- raw_data %>%
  filter(!is.na(created_year) & !is.na(created_month) & !is.na(created_date) & created_year >= 2005)
```

  Overall, only 6 rows of data were deleted, which does not affect the subsequent analysis because it only accounts for 0.6% of the data.


### 3.1.4 Handle repeating lines
```{r Inpsect repeat lines}
# Use unique function handle repeating lines
raw_data_unique <- unique(raw_data)
raw_data <- raw_data_unique
rm(raw_data_unique)
```


### 3.1.5 Handle outliers

#### 3.1.5.1 classify variables
  Change the variables into numerical and categorical according to the data type.
```{r classify variables}
numeric_vars <- raw_data[sapply(raw_data,is.numeric)]
categorical_vars <- raw_data[sapply(raw_data,function(x) class(x) %in% c('factor','character'))]
numeric_vars_name <- names(numeric_vars)
categorical_vars_name <- names(categorical_vars)
```

#### 3.1.5.2 Identify outliers
  Create a function to identify outliers for all variables in the dataset
```{r Outliers detection}
# Define a function to detect outliers in the dataset
detect_outliers <- function(data, method = "zscore", threshold = 3) {
  # Initialize an empty dataframe to store detected outliers
  outliers <- data.frame()
  # Loop over each column of the dataset
  for (col in colnames(data)) {
    # Check if the column is numeric as outliers can be detected only in numeric columns
    if (is.numeric(data[[col]])) {
      # If method is zscore, then use the z-score method to detect outliers
      if (method == "zscore") {
        z <- (data[[col]] - mean(data[[col]])) / sd(data[[col]])
        # If the z-score value exceeds the threshold, consider it as an outlier
        outliers <- rbind(outliers, data[abs(z) > threshold, ])
      # Else, if method is iqr, then use the Interquartile Range (IQR) method to detect outliers
      } else if (method == "iqr") {
        Q1 <- quantile(data[[col]], 0.25)
        Q3 <- quantile(data[[col]], 0.75)
        IQR <- Q3 - Q1
        # If the value is outside the range defined by Q1 - 1.5*IQR and Q3 + 1.5*IQR, consider it as an outlier
        outliers <- rbind(outliers, data[data[[col]] < (Q1 - 1.5 * IQR) | data[[col]] > (Q3 + 1.5 * IQR), ])
      }
    }
  }
  return(outliers)
}
# Use the detect_outliers function to detect outliers in columns 1 to 18 of the Youtube_cleaned dataset
find_outliers <- detect_outliers(raw_data[numeric_vars_name])
# Print the detected outliers
print(find_outliers)
rm(find_outliers)
```

#### 3.1.5.3 Winsorize outliers
  To deal with extreme values in the dataset, we used the method of Winsorizing (shrinking the tails), which sets values greater than the 95th percentile in the dataset to the 95th percentile value. This method has unique advantages over other methods of dealing with outliers, such as truncation or deletion. 
  First, it preserves much of the original information of the data. Second, truncation reduces the potential adverse effects of extreme values or outliers on the model, thus improving the stability and predictive accuracy of the model. 
  In addition, some studies have also shown that the results of statistical hypothesis validation and model fit of the model can be effectively improved by appropriate outlier treatment [@Dixon1960].
  
First,identify which variables are numerical or categorical.并且设置好Category列的变量和Numeric列的变量

```{r Winsorize outliers}
# Function to truncate values in specified columns that exceed a given percentile.
truncate_tail <- function(data, column_names, percentile = 95) {
  
  # For each column, replace values above the specified percentile with the value at that percentile.
  for (col in column_names) {
    q <- quantile(data[[col]], probs = percentile / 100, na.rm = TRUE)
    data[complete.cases(data) & data[[col]] > q, col] <- q
  }
  
  return(data)
}

# Apply the function to the first 18 columns of Youtube_cleaned.
raw_data <- truncate_tail(raw_data, numeric_vars_name, percentile = 95)

```

  Secondly,summarize how much outliers in each variable
```{r Outliers Count}
# Function to compute z-scores and detect outliers based on a specified threshold
detect_outliers_zscore <- function(data, column_names, threshold = 3) {
  outliers_count <- sapply(column_names, function(col_name) {
    # Calculate z-scores for each specified column
    z_scores <- (data[[col_name]] - mean(data[[col_name]], na.rm = TRUE)) / sd(data[[col_name]], na.rm = TRUE)
    # Count the number of absolute z-scores that exceed the provided threshold
    sum(abs(z_scores) > threshold, na.rm = TRUE)
  })
  return(outliers_count)
}

# Detect outliers in the data after truncation using the function above
outliers_count_after_truncate <- detect_outliers_zscore(raw_data, numeric_vars_name)
print(outliers_count_after_truncate)
rm(outliers_count_after_truncate)
```
  Ultimately, we observe a reduction in the impact of outliers, but it's impractical to address all of them. Overcorrecting could potentially distort the data. With the cleaning phase now complete, we're ready to transition to the classification stage.


### 3.1.6 Change data type 
  Change data type from character to factor for 'category', 'country', 'channel type'
  Change some categorical variable to factors to prepare for Classification
```{r Factor type}
factor_columns <- c("category", "country","country_abbr", "channel_type")
#Use mutate adjust factor_columns, change then to factor type
raw_data <- raw_data %>%
  mutate(across(all_of(factor_columns), as.factor))
rm(factor_columns)
```


###3.1.7 Backup the processed data 
Backup the raw_data into another name youtube_data
```{r Backup Data}
youtube_data <- raw_data
```


### 3.1.8 Add new columns
  Add new columns which might be used in the future.

#### 3.1.8.1 Full time column
```{r full time}
# First, ensure that the 'created_date' column is in integer format
# Assuming the data doesn't contain any decimal parts or NA values
youtube_data$created_date <- as.integer(as.character(raw_data$created_date))

# Check for any non-integer values in 'created_date'
if (any(!is.finite(youtube_data$created_date))) {
  stop("The 'created_date' column contains non-integer or NA values.")
}

# Define a vector of month names
month_names <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Create a vector of two-digit month numbers using sprintf
month_numbers <- sprintf("%02d", 1:12)

# Create a mapping between month names and month numbers
month_mapping <- setNames(month_numbers, month_names)

# Update the 'full_date' column by combining year, month, and day
youtube_data$full_time <- as.Date(paste(raw_data$created_year, 
                                   raw_data$created_month %>% match(month_names) %>% month_mapping[.],
                                   sprintf("%02d", youtube_data$created_date), 
                                   sep = "-"), 
                             format = "%Y-%m-%d")
rm(month_mapping, month_names, month_numbers)
```

#### 3.1.8.2 Subscriber engagement column
  video_views/subscribers. This can give an idea of how many videos, on average, each subscriber watches, offering a measure of engagement for the channel.
```{r Subscriber engagement}
youtube_data$subs_engagement <- youtube_data$video_views/youtube_data$subscribers
```

#### 3.1.8.3 Add video upload frequency column
  uploads/(2023-created_year). This can show how many videos a YouTuber uploads on average each year since they created their channel.
```{r video upload frequency}
youtube_data$uploads_frequencey <- youtube_data$uploads/(2023-youtube_data$created_year)
```

#### 3.1.8.4 Average views per video column
  video_views/uploads. This can help in understanding the average number of views per video, offering a measure of the content's popularity.
```{r Average views per video}
youtube_data$average_views_per_video <- youtube_data$video_views/youtube_data$uploads
```

#### 3.1.8.5 Urbanization rate column
  urban_population/population. This will give a percentage of urbanization in a country.
```{r  Urbanization rate}
youtube_data$urban_rate <- youtube_data$urban_population/youtube_data$population
```

#### 3.1.8.6 Years of experience column
  2023 - created_year. The number of years since the channel was created, which might relate to income and popularity.
```{r Years of experience}
youtube_data$age <- 2023-youtube_data$created_year
```

#### 3.1.8.7 Education population ratio column
  tertiary_edu_enrollment/population. This gives a ratio of the population that's undergoing higher education in each country
```{r Education population ratio}
youtube_data$edu_ratio <- youtube_data$tertiary_edu_enrollment/youtube_data$population
```

#### 3.1.8.8 Top channel flag column
  A binary feature based on rank (if channel rank < 1000 then 1, else 0).
```{r Top channel flag}
youtube_data$top_channel_flag <- as.numeric(youtube_data$channel_type_rank <1000)
```

#### 3.1.8.9 Video views increasing rate column
  video_views_last_30_days/ video_view. This shows the increasing rate about video views.
```{r video_views_increasing_rate}
youtube_data$video_views_increasing_rate <- (youtube_data$video_views_last_30_days/youtube_data$video_views)
```

#### 3.1.8.3 Add average earning columns
  Average earning for each month and each year.
```{r Average Earnings}
youtube_data$monthly_earning <-( youtube_data$monthly_earning_high+youtube_data$monthly_earnings_low)/2
youtube_data$yearly_earning <-( youtube_data$yearly_earning_low+youtube_data$yearly_earning_high)/2
```

  Observation found that among the overall monthly income and annual income, there are rows where the monthly income is much greater than the annual income. The reason for this problem is that some rows with the lowest monthly income have na values, which turned into average values during previous processing, resulting in this unreasonable situation.
  Create a function to identify all these unreasonable rows
```{r Detect unreasonable earnings rows}
detect_unusual_rows <- function(data) {
  unusual_rows <- data[data$monthly_earning > data$yearly_earning, c("monthly_earning", "yearly_earning","monthly_earnings_low","monthly_earning_high","yearly_earning_low","yearly_earning_high")]
  print(unusual_rows)
  cat("The number of unreasonable rows are", nrow(unusual_rows), "rows\n")
}
detect_unusual_rows(youtube_data)
```
  It can be found that these unreasonable rows account for less than 5% of all rows.To ensure the data integrity of classification,these rows can be deleted. According to the discussion with Sirui Liu in Teams, it is acceptable to delete rows less than 5% in total. For now we delete 45 rows(4.6%).
```{r Delete Unreasonable Rows}
youtube_data <- youtube_data[youtube_data$monthly_earning <= youtube_data$yearly_earning, ]
```

## 3.2 Data Preparation


### 3.2.1 Object and features

In this project, we would build model and predict the high earning Youtuber on the top list.
For now we pick some features like:
    "subscribers","video_views","uploads","channel_type","created_year",
    "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate"
Category：high earning vs low earning
All the features and category variables have been prepared in 3.1.8 Section.


Data Integration：
  We would use the World Bank GDP data from the Project2 prject description, which includes GDP per capita for each country for each year. Based on the country field merge the GDP for 2022 year. We decided to use 2022 year GDP for each country as a reference. Because our Youtuber data is for 2023, 2022 is the closest year to 2023 in the GDP data, which ensures that real-word economic fluctuations will have the least impact because the years are closest.
  After manually check, we realized that the first 4 rows of the GDP csv file are not data columns, so we skipped the first 4 rows using the skip parameter. When loading, our GPD data would only load up to the 2005 GDP and could not load the rest right side columns. So we solved the problem by using ChatGPT4 to learn how to select specific columns using the readr package.
```{r gdp data path,echo=FALSE}
gdp_data_path <- "API_NY.GDP.PCAP.CD_DS2_en_csv_v2_5871588.csv"
# 使用 cols_only 函数为 read_csv 指定所需的列
desired_columns <- cols_only(`Country Name` = col_character(), `2022` = col_double())

gdp_data <- read_csv(gdp_data_path, skip = 4, col_types = desired_columns)
rm(desired_columns)

gdp_data <- gdp_data %>%
  rename(gdp_per_capita = "2022")
```

Use merge command here.
```{r merge gdp data}
gdp_2022 <- gdp_data[,c("Country Name","gdp_per_capita")]

youtube_data <- merge(youtube_data,gdp_2022, by.x = "country",by.y = "Country Name", all.x = TRUE)
rm(gdp_2022)
```

  If the country name is "Unknown", then we would use the medium of youtube_data$gdp_per_capita to swap it, it won't affect our data too much, and also, this is the same handling method that we used before in 3.1.3 Handle Missing value. Consistency is ensured here.
```{r handle unknown country}
median_gdp <- median(youtube_data$gdp_per_capita,na.rm = TRUE)
youtube_data$gdp_per_capita[is.na(youtube_data$gdp_per_capita)] <- median_gdp
rm(median_gdp)
```


Normalize Youtuber earning： 
  We start by using Youtuber yearly earning divided by GDP per capita to observe how many time the Youtuber's income is the country's per capita income. Here we use annual income divided by GPD per capita, the reason for using average annual income rather than average monthly income is that anuual income is more stable and reduces fluctuations in income due to seasonality or other short term factors. Secondly, annual income is more objective when compared to annual GDP per capita.
```{r normalized earning}
youtube_data$normalized_earning <- as.numeric(youtube_data$yearly_earning/youtube_data$gdp_per_capita)
```


Logic for defining high income:
    We would like to use the mean and add a certain number of standard deviations as a threshold for high income based on the standard deviation, but with this approach we have to first assume that income is normally distributed, which is not always the case in reality.
    
    
Validation Definition:
  We want to verify that our data is normally distributed by visualizing a graph.
```{r check normalized earning}
summary(youtube_data$normalized_earning)
```
As we can see there is a huge gap between the median and the mean. So we continue with kernel density estimates, and statistical tests, here we used the Shapiro-Wilk test; the Kolmogorov-Smirnov test; and the Anderson-Darling test
```{r normalization test}
#Kernel Density Estimation
plot(density(youtube_data$normalized_earning), main="Kernel Density Estimation of Normalized Earnings", xlab="Normalized Earnings")

# Shapiro-Wilk test
shapiro_test <- shapiro.test(youtube_data$normalized_earning)
print(shapiro_test)
# Kolmogorov-Smirnov test
ks_test <- ks.test(youtube_data$normalized_earning, "pnorm", mean(youtube_data$normalized_earning), sd(youtube_data$normalized_earning))
print(ks_test)
# Anderson-Darling test
ad_test <- ad.test(youtube_data$normalized_earning)
print(ad_test)
```
Shapiro-Wilk normality test.
  The value of W is 0.44508.The value of this statistic ranges from 0 to 1, where 1 indicates that the data perfectly fits a normal distribution. However, the resulting W value is much less than 1, which is an initial indication that the data may not be normally distributed. Moreover, the p-value is very close to 0 (less than 2.2e-16), which means that we should reject the original hypothesis of normality.
Kolmogorov-Smirnov test.
  The D-value is 0.34033. this is the largest gap between the Earning data distribution and the perfect normal distribution. Again, the p-value is very small (less than 2.2e-16) which further confirms that the data is not normally distributed. The test also gives a warning about the presence of duplicate values (ties) in the data. This is because we used the median instead of the Youtuber for the unknown country.
Anderson-Darling normality test.
  The value of A is 191.44. the larger this value, the more the data deviates from the normal distribution. As with the previous two tests, a very small p-value (less than 2.2e-16) indicates that the data are not normally distributed.
Summary: 
  All three tests consistently show that the youtube_data$normalized_earning data does not currently conform to a normal distribution.


Adjustment and Optimization:
  While modern machine learning algorithms do not require the data to meet the assumption of a normal distribution. However, bringing the data closer to a normal distribution can improve model performance, especially for models such as linear regression and logistic regression.
  We observe some very large values from the density plot, they are much larger than thousands of times the average GDP, and for these values they are simply too different. We will use a logarithmic transformation, which is what we learned from class to reduce the effect of these too large values. After that the multiples were adjusted to between 0-10, and according to the Density image, it's also leaning more towards a normal distribution.
```{r log transformation}
youtube_data$normalized_earning <- log(youtube_data$normalized_earning + 1)  
```

```{r Check normalized earning}
summary(youtube_data$normalized_earning)
# Shapiro-Wilk test
shapiro_test <- shapiro.test(youtube_data$normalized_earning)
print(shapiro_test)

# Kolmogorov-Smirnov test
ks_test <- ks.test(youtube_data$normalized_earning, "pnorm", mean(youtube_data$normalized_earning), sd(youtube_data$normalized_earning))
print(ks_test)

# Anderson-Darling test
ad_test <- ad.test(youtube_data$normalized_earning)
print(ad_test)
# Density plot
plot(density(youtube_data$normalized_earning), main="Kernel Density Estimation of Normalized Earnings", xlab="Normalized Earnings")

rm(ad_test,ks_test,shapiro_test)
```

Shapiro-Wilk Test.
  The w-value is 0.98524, which is close to 1. This is usually a good sign that the data is close to a normal distribution. However, the p-value = 3.358e-08 (very small) is much less than 0.05
Kolmogorov-Smirnov Test.
  D-value is 0.038866 which is small. p-value = 0.1134 which is greater than 0.05
Anderson-Darling Test.
  A value of 2.2485, large value. p-value = 1.056e-05 (very small), much less than 0.05

  youtube_data$normalized_earning is close to a normal distribution, although it is not exactly normal. Many real-world applications have data that do not strictly follow a normal distribution, but if they are close enough, many of the properties of a normal distribution are still useful.
  When defining high income based on mean and standard deviation, we choose the mean plus one times the standard deviation: about 84% of the data points will be less than this value. Thus, this would be a relatively loose definition of high income that includes about 16% of YouTubers.
```{r}
earning_threshold <- median(youtube_data$normalized_earning)+sd(youtube_data$normalized_earning)
print(earning_threshold)
```
It means if a Youtuber's yearly earning is 6.2 times larger than GDP per capita then this Youtuber is a high earning Youtuber.


### 3.2.2 Remove unrelevant columns
  According to the instructions of project 2.Columns that have a unique value for each row should be discarded, e.g., rank, Youtuber, etc.List all the columns that have a unique values for each row and then delete those columns.
```{r Remove unrelevant colunmns}
youtube_data <- youtube_data[
  c(
    "normalized_earning",
    "subscribers","video_views","uploads","channel_type","created_year","category",
    "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate"
    )
  ]
```


### 3.2.3 Feature Engineering
We have already created required features in 3.1.8 Section.
  "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate"
  Here, we would use 6.2 times as threshold, build a new binary column, if Youtuber earning is larger than 6.2 return 1, else 0.
```{r}
youtube_data$earning_class <- ifelse(youtube_data$normalized_earning > 6.2, 1, 0)
youtube_data <- subset(youtube_data, select = -normalized_earning)
```


```{r}
names(youtube_data)
```
### 3.2.4 Training and Testing dataset 
  According to the different income levels, we need to split the youtube_data into training data and testing data. We do a 20/80 split to form the training and test sets.
  After google many articles and learn caret pacakge by ourselves, we decide to use "creatDataPartition" function in caret package, it ensures that the distribution of the target variable is taken into account when partitioning the data.[@Kuhn2019]
```{r}
#Set a seed 
set.seed(562816)

train_index <- createDataPartition(youtube_data$earning_class, p=.8,list=FALSE,times=1)

youtube_train <- youtube_data[train_index,]
youtube_test <- youtube_data[-train_index,]
```








references.bib
title: 
output: html_document
bibliography: references.bib

# Reference
@Online{Elgiriyewithana2023,
  author = {Nidula Elgiriyewithana},
  title = {Global YouTube Statistics 2023},
  year = {2023},
  month = {Aug},
  url = {https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023},
  note = {Accessed: yyyy-mm-dd} % replace with the actual access date
}


@Manual{Wickham2016,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Hadley Wickham},
  year = {2016},
  publisher = {Springer-Verlag New York},
  note = {ISBN 978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}

@Manual{Wickham2021,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.7},
  url = {https://dplyr.tidyverse.org},
}


@Article{Friendly2002,
  author = {Michael Friendly},
  title = {Corrgrams: Exploratory displays for correlation matrices},
  journal = {The American Statistician},
  year = {2002},
  volume = {56},
  number = {4},
  pages = {316–324},
}

@Manual{McNamara2021,
  title = {skimr: Compact and Flexible Summaries of Data},
  author = {Amelia McNamara and Hao Zhu and Eduardo Arino de la Rubia and Shannon Ellis and Julia Lowndes and Michael Quinn},
  year = {2021},
  note = {R package version 2.1.3},
  url = {https://cran.r-project.org/web/packages/skimr/},
}


@Book{Healy2018,
  author = {Kieran Healy},
  title = {Data Visualization: A Practical Introduction},
  publisher = {Princeton University Press},
  year = {2018},
}


@Article{JoanesGill1998,
  title = {Comparing measures of sample skewness and kurtosis},
  author = {D. N. Joanes and C. A. Gill},
  year = {1998},
  journal = {The Statistician},
  volume = {47},
  number = {1},
  pages = {183-189}
}

@online{Kuhn2019,
  author    = {Max Kuhn},
  title     = {The caret Package 4.1 Simple Splitting Based on the Outcome},
  year      = {2019},
  url       = {http://topepo.github.io/caret/data-splitting.html},
  note      = {Accessed: 2023-10-11}
}



1.Introduction
2.Setup
3.Data Preparation
4.Classification
4.1 Target Variable
4.2 Feature Variables
4.3 Test and Training Sets
4.4 Null model
4.5 Single Variable Model
4.6 Model Evaluation
4.7 Naive Bayes
4.8 Logistic Regression
4.9 Comparison

5 Clustering
 - 5.1 Hierarchical Clustering
5.2 Optimal Number of Clusters
5.3 Validating Clusters
5.4 Exploring Clusters
5.4.1 Job Locations
5.4.2 Salary
5.4.3 Rating

