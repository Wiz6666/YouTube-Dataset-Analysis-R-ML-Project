---
title: "CITS4009-project2"
author: "Wiz ZHANG(SID:23210735)； Xiao ZHANG"

output:   
  html_document:
    highlight: haddock
    theme: cosmo
    code_folding: hide
---
```{r setup, include=FALSE}
#Set all code chunks to display their R code by default
knitr::opts_chunk$set(echo = TRUE)
```


收入级别分类：

问题：博主的收入水平属于什么档次
特征：订阅者数、视频观看数、上传数、创建年份、视频观看增长率并且结合年收入和月收入进行判断，还有可以加上channel type(根据type进行纵向比较)。
类别：高收入  vs. 低收入


# Part1 - Introduction
## 1.1 - Project Background and Purpose
  In this project, our objective is to illustrate the process of modeling by using R machine learning functions. We aim to understand and apply various stages of data science including data preparation, model building, and model evaluation. We have choosen the YouTube dataset which is shared by uni coordinator. We also use the same dataset in our Project 1. The specific contributions of each member will be separated 50-50 fairly.
  
## 1.2 Data Source and Characteristics
  The data set analyzed can be obtained from Kaggle platform. It comes from the "Global YouTube Statistics 2023". (https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023)
  A collection of YouTube giants, this dataset offers a perfect avenue to analyze and gain valuable insights from the luminaries of the platform. With comprehensive details on top creators' subscriber counts, video views, upload frequency, country of origin, earnings, and more, this treasure trove of information is a must-explore for aspiring content creators, data enthusiasts, and anyone intrigued by the ever-evolving online content landscape. Immerse yourself in the world of YouTube success and unlock a wealth of knowledge with this extraordinary dataset.[Elgiriyewithana2023]





# Part2 - Setup

## 2.1 Load required R packages

  We load 'ggplot2' package [@Wickham2016] for making static graphics and 'dplyr' package [@Wickham2021] for data manipulation.
  We employ scatterplot matrices and correlation heatmaps for exploratory data analysis to understand the pairwise relationships between variables [@Friendly2002]...
  We use the skimr package to obtain more comprehensive descriptive statistics of the numeric variables in the dataset [@McNamara et al.2021]...
  
```{r Library, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggthemes)
library(numform)
library(treemapify)
library(timeDate)
library(lubridate)
library(reshape2)
library(ca)
library(skimr)
library(janitor)
library(flextable)
library(shiny)
library(leaflet)
library(maps)
library(wordcloud)
library(RColorBrewer)
library(scales)

```

## 2.2 Set plot theme 

  The theme is set up to enhance the readability and interpretability of the graphs, following the best practices in data visualization [@Healy2018].
```{r Theme}
project_theme <- theme(
  panel.background = element_rect(fill = "#FFFBDC"),  # Light yellow background
  panel.grid.major = element_line(color = "#FFE4A1"), # Light orange major grid lines
  panel.grid.minor = element_blank(), # Remove minor grid lines
  plot.title = element_text(size = 18, hjust = 0.5, color = "darkblue"),  # Title color and size
  axis.title = element_text(size = 16, color = "darkblue"),  # Axis title color and size
  axis.text = element_text(size = 14, color = "black"),   # Axis text color and size
  legend.title = element_text(size = 16, color = "darkblue"), # Legend title color and size
  legend.text = element_text(size = 14, color = "black"),   # Legend text color and size
  legend.background = element_rect(fill = "#FFFBDC"),  # Legend background color
  plot.background = element_rect(fill = "#FFFBDC")   # Background color of the entire plot
)
```

## 2.3 Load the main data
  In this part, we will load the main dataset, store the path in 'Data_path'
```{r readfile, warning=FALSE}
data_path <- 'Global YouTube Statistics.csv'
raw_data <- read.csv(data_path,encoding = "UTF-8")
```

## 2.4 Dataset initial overview
  Before starting in-depth analysis and modeling, it is crucial to perform an initial overview of the dataset. By using functions such as summary, str, etc., we can gain an overall understanding of the data, including data types, data distribution, missing values and possible outliers.
    This is the process that we already done in Project 1, according to Professor Wei, we don't need to do it again so we pass here. So we shorten the content about dataset overview. Mainly is using str; summary;head command to check dataset overview, we also use a function to check dataset skewness. We also shorten or even skip the EDA part in Project 2 report.
  First, we use str command to analyze the data
```{r str,results='hide', echo=FALSE, warning=FALSE}
str(raw_data)
```

The "Global YouTube Statistics" dataset contains 995 observations and 28 variables.
  Then we would use summary command to analyze data. 
  we use skimr here, which is similar to summary command but it shows more descriptive statistics for numeric variables than summary command.
```{R Skimr Summary,results='hide', echo=TRUE, warning=FALSE}
skimr::skim(raw_data) |>
  flextable::flextable() 
```

  We can see from the standard output that 'video_views_rank' has 1 missing value, country_rank has 116, 'subscribers_for_last_30_days' has 337 the most in the whole dataset.
  The numbers of missing value of 'Gross.tertiary.education.enrollment.rate','Population', 'Unemployment.rate', 'Urban_population', 'Latitude' are the same , so it should be a systematic missing issue. So we need to pay attention to this issue in the following data cleaning.
  According to the numeric.hist, it seems most of the numeric variables are right skewnessed.The 'Unemployment.rate' does not look like a normal distribution,so as 'Longitude'.
  Skewness analysis is crucial as it helps in understanding the symmetry of the data distribution [@Joanes & Gill1998]. We observe a higher percentage of right-skewed columns in our dataset, which implies that most numerical variables in our dataset have a distribution that has a tail on the right side, meaning the right side of the distribution is longer or fatter than the left side. This can impact the analysis and model performance because many statistical methods assume a normal distribution of the data. Right-skewed distributions are associated with higher values and outliers on the right side of the distribution which can greatly impact the mean and variance, and subsequently, distort the interpretation and performance of the model.
  Addressing skewness is often important in regression models where the assumption of normally distributed residuals is made. Therefore, recognizing the presence of skewness can help us make informed decisions about data transformation techniques, such as logarithmic transformation or Box-Cox transformation, to attempt to normalize the data distribution before applying any modeling techniques. For example, this can be particularly relevant for variables like `subscribers`, `uploads`, and `lowest_monthly_earnings`, where skewness might impact the model’s ability to generalize well to unseen data and might lead to unreliable predictions or insights.
  
  Calculate Right skewness numbers and left skewness numbers
```{r Skewness Calculator}
calculate_skewness_percentage <- function(dataframe) {
  #initialize function variables
  right_skewed_count <- 0
  left_skewed_count <- 0
  column_names <- names(dataframe)
  #count the numbers with loops
  for (column in column_names) {
    if (is.numeric(dataframe[[column]])) {
      median_value <- median(dataframe[[column]], na.rm = TRUE)
      mean_value <- mean(dataframe[[column]], na.rm = TRUE)
      if (mean_value > median_value) {
        right_skewed_count <- right_skewed_count +1
      } else if (mean_value < median_value) {
        left_skewed_count <- left_skewed_count +1
      }
    }
  }
  #calculate percentage with the count result
  right_percentage <- round((right_skewed_count / ncol(dataframe)) * 100, 2)
  left_percentage <- round((left_skewed_count / ncol(dataframe)) * 100, 2) 
  
  return(list(right_percentage, left_percentage))  
}

# Use function to get the calculation result
results <- calculate_skewness_percentage(raw_data)
cat("\n")
cat("Percentage of right-skewed columns is:", results[[1]], "%\n")
cat("Percentage of left-skewed columns is:", results[[2]], "%\n")
rm(results)
```
  The outcomes of our skewness analysis are telling and significant, revealing that 50% of the columns in our dataset are right-skewed, in contrast to a mere 21.43% being left-skewed. This preponderance of right-skewed columns is pivotal as it unveils a tendency for the distribution of most numerical variables to be tail-heavy on the right side, a characteristic that might have consequential implications for subsequent analyses and modeling efforts.
  The presence of such skewed columns underlines the existence of numerous higher values and potential outliers on the right side of the distribution. These extreme values have the capability to significantly inflate the mean and variance of the data, potentially leading to distorted interpretations and undermining the reliability and robustness of our subsequent statistical models.
  The substantial right skewness observed in specific variables such as subscribers, uploads, and lowest_monthly_earnings could particularly impact the predictive power and generalizability of the models. Such skewness could render the models less resilient and adaptive to unseen or new data, thereby possibly yielding unreliable predictions or insights.
  Employing such transformations could help mitigate the impact of skewness and align the data more closely with the assumptions of many statistical methods, predominantly enhancing the reliability and validity of our subsequent analyses and models.

  Then we use head command to keep analyzing data.
```{r head, results='hide', echo=FALSE, warning=FALSE}
head(raw_data,n=100)
```
  Base on the standard output, we can see that subscribers shows how many people subscribe the Youtuber. The Title is the same as Youtuber, we need to figure out why it is the same but in two different columns. the Music Youtuber has 119000000 subsribers but only 0 uploads , so we need to make data cleaning later.





# Part3 - Data Preparation and Preprocessing



## 3.1 Data Cleaning

### 3.1.1 Check column names and make adjustments

  First, we use names() command to check each variable name.
```{r names,results='hide'}
names(raw_data)
```

  Then , we use janitor package and rename function adjust variable names to ensure uniform formatting.
  
  janitor::clean_names() is a function in the package whose main purpose is to convert the column names of dataframes into a clean, uniform format. Here is the main logic and rules of this function:
1.All characters to lowercase
2.Spaces are converted to underscores
3.Non-alphanumeric characters are removed
4.Numbers before text moved to the end
5.Underline after keyword in R)
```{r Name Changer}
raw_data <- janitor::clean_names(raw_data)
```

  From the output result we can see most of the variable names have been adjusted properly. But some of them still need manual adjustment for better understanding and clearness.
```{r Name Changer2}
raw_data <- raw_data %>%
  rename(
    #More descriptive names
    video_views_last_30_days = video_views_for_the_last_30_days,
    country_abbr = abbreviation,
    #Shorter names
    monthly_earnings_low = lowest_monthly_earnings,
    monthly_earning_high = highest_monthly_earnings,
    yearly_earning_low = lowest_yearly_earnings,
    yearly_earning_high = highest_yearly_earnings,
    subs_last_30_days = subscribers_for_last_30_days,
    tertiary_edu_enrollment = gross_tertiary_education_enrollment,
    )
print(colnames(raw_data))
```
  
### 3.1.2 Inspect & Adjust data types
Use sapply function to check data types, adjust wrong data typs.
```{r Data type, results='hide', echo=FALSE,warning=FALSE}
data_types <- sapply(raw_data,class)
data_sample <- sapply(raw_data,head)
print(data_types)
print(data_sample)
rm(data_types, data_sample)
```

  From the standard output first, we know that we need to handle the missing values; 'created_month' should change from char type to numeric type this is useful for following analysis; 'category','country','channel_type','abbreviation' shoule be factor type which is better for following analysis;handle the outliers according to the skimmr result.
  
### 3.1.3 Handle Missing value
  To address missing values, we have observed three types of characters representing missing data: 0, "NaN," and "nan." Our objective is to convert these representations into NA and subsequently count the number of NA values in each row. 
先把特殊值（0 ；NaN；nan）转换NA，紧接着计算每一列的NA数量。我们主观认为0是指NA的哨兵值。
```{r Kill 0&NA and delete NA}
#Change 0 or 'nan' or 'NaN' to NA value
#Char columns
raw_data[raw_data == 'nan' | raw_data == "NaN" | raw_data == 0] <- NA
#Numeric columns
raw_data <- raw_data |>
  mutate(across(where(is.numeric), ~ifelse(is.nan(.), NA, .)))
```

使用函数计算每一列的NA值
  Use function to calculate how many missing values in each column, handle them.
```{r col_NA calculator}
# Calculate missing value number
count_missing <- function(df) {
  sapply(df, FUN = function(col) sum(is.na(col)) )
}
# Calculate percentage of missing value
percentage_missing <- function(df) {
  sapply(df, FUN = function(col) round(sum(is.na(col)) / length(col) * 100, 2) )
}

nacounts <- count_missing(raw_data)
napercents <- percentage_missing(raw_data)

# output the result
hasNA = which(nacounts > 0)
data.frame(Column = names(nacounts[hasNA]), 
           Missing_Values = nacounts[hasNA], 
           Percentage_Missing = napercents[hasNA])
rm(col,hasNA,nacounts,napercents)

```
  Based on the results, it's evident that many columns contain a significant number of missing values. However, our dataset only consists of 995 rows. If we were to delete these missing values extensively, we might end up with an insufficient amount of data for subsequent modeling. Therefore, we've devised different approaches to handle different types of variables.

  Before addressing the missing values, it's essential to identify the variables that we won't be processing and provide an explanation.
Firstly, we've chosen not to process the 'abbreviation' column. Despite being able to replace its missing values with 'unknown,' the sheer quantity of missing values (122 in total) makes it impractical to proceed with this column.
  Additionally, we've decided not to handle the 'latitude' and 'longitude' columns. These columns lack provided geographical information, and the proportion of missing values exceeds 10%, which is relatively high. Consequently, these two variables aren't suitable for the upcoming modeling process.
  Lastly, we won't be dealing with the 'rank,' 'subscribers,' 'Title,' and 'Youtuber' columns. This decision is based on the fact that these columns do not contain any missing values.
  Now, we need to decide on the methods to use for different columns. First, let's consider the 'video views' column, which has few missing values. We can choose between deletion or imputation with the median. However, since we want to maintain an adequate number of rows for modeling, we opt for replacing the missing values with the median.As for the 'Earnings' variable, it follows a skewed distribution. Therefore, we plan to use median imputation.Regarding the time-related variables, they cannot be inferred from other variables, and they are not continuous time series data, making ARIMA modeling inapplicable. Fortunately, there are only 5 rows with missing values, accounting for just 0.5% of the entire dataset. Our chosen approach is to delete rows with missing time values and not impute them.

  In summary, the numeric variables that require processing are "video_views",
"uploads","video_views_rank","country_rank","channel_type_rank","video_views_last_30_days","monthly_earnings_low",	"monthly_earning_high","yearly_earning_low",	 "yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate","urban_population", 
and the categorical variables to be addressed are "category", "country","country_abbr","channel_type".

我们发现很多行是大面积丢失，
首先，我们只有995 rows的数据，所以我们必须尽力不要删除任何行，否则我们没有足够的数据进行接下来的分析。我们决定插补数据，对于分类变量我们使用Unknown 填补，代表缺失

1. Video Views
缺失值很少，考虑删除或者中位数插补。但是我们尽量不删除，切数据偏态分布，所以使用中位数填补。
2.Country & Abbreviation
 虽然可以使用Unknown 更换缺失值，但是总共缺失122个数值，数据缺失过大不适用于接下来的分析工作
3. Earnings
由于是偏态分布的数据，考虑中位数插补
4.经纬度
没有可以使用的地理信息，缺失占10%以上，这里考虑无视。不进行到接下来的机器学习分析当中
5.时间变量
无法从其他变量推断，不是连续的时间序列数据，无法使用ARIMA模型推断，但是只有5 rows是缺少的，只占了全部数据的0.5%，删除没有时间的行, 不填补时间的缺失值
6.不处理rank；subscribers；Title;Youtuber没有发现缺失值

          
需要处理的数字类型列：
c("video_views",
"uploads","video_views_rank","country_rank","channel_type_rank","video_views_last_30_days","monthly_earnings_low",	"monthly_earning_high","yearly_earning_low",	 "yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate","urban_population")	

所有文本类型列：
c("category", "country","country_abbr","channel_type")


我们首先处理以上数字类型列。使用中位数填充缺失值
  First, we are going to handle numeric variable, using medium to replace the NA
  Base on the result of skewness test, most of data is right skewed, so we should use medium which is better than using mean value, because mean value would change the right skewed structure.
Here, I change the columns where more than 10% of data is NA.
```{r Fill with medium}
change_to_median_cols <- c("video_views",
"uploads","video_views_rank","country_rank","channel_type_rank","video_views_last_30_days","monthly_earnings_low",	"monthly_earning_high","yearly_earning_low",	 "yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate","urban_population")
for (col in change_to_median_cols) {
    median_val <- median(raw_data[[col]], na.rm = TRUE)
    raw_data[[col]][is.na(raw_data[[col]])] <- median_val
}
rm(col,median_val,change_to_median_cols)
```


这里将需要处理的文本列使用“Unknown”转换NA值
  Secondly, we change the NAs in cateogrical variables into "Unknown"
```{r Unknown changer}
character_columns <- c("category", "country","country_abbr","channel_type")
  
for (col in character_columns) {
  raw_data[[col]][is.na(raw_data[[col]])] <- "Unknown"
}
rm(col,character_columns)
```

  Delete all the missing rows (5 rows) in the time column. Since there are only 5 rows, it does not affect the subsequent analysis. At the same time, delete the rows with the channel creation year less than 2005, because the YouTube company was created in 2005.
删除所有时间列是缺失的行（5行），由于只有5行不影响接下来的分析，同时删除频道创建年份小于2005的行，因为youtube公司的创建时间是2005年.
```{r}
raw_data <- raw_data %>%
  filter(!is.na(created_year) & !is.na(created_month) & !is.na(created_date) & created_year >= 2005)
```

  Overall, only 6 rows of data were deleted, which does not affect the subsequent analysis because it only accounts for 0.6% of the data.
总共删除了6行数据，并不影响接下来的分析，只占了数据的0.6%


### 3.1.4 Handle repeating lines
```{r Inpsect repeat lines}
# Use unique function handle repeating lines
raw_data_unique <- unique(raw_data)
raw_data <- raw_data_unique
rm(raw_data_unique)
```
### 3.1.5 Handle outliers
#### 3.1.5.1 classify variables
Change the variables into numerical and categorical according to the data type.
```{r}
numeric_vars <- raw_data[sapply(raw_data,is.numeric)]
categorical_vars <- raw_data[sapply(raw_data,function(x) class(x) %in% c('factor','character'))]
numeric_vars_name <- names(numeric_vars)
categorical_vars_name <- names(categorical_vars)
```
#### 3.1.5.2 Identify outliers
Create a function to identify outliers for all variables in the dataset
```{r Outliers detection}
# Define a function to detect outliers in the dataset
detect_outliers <- function(data, method = "zscore", threshold = 3) {
  # Initialize an empty dataframe to store detected outliers
  outliers <- data.frame()
  # Loop over each column of the dataset
  for (col in colnames(data)) {
    # Check if the column is numeric as outliers can be detected only in numeric columns
    if (is.numeric(data[[col]])) {
      # If method is zscore, then use the z-score method to detect outliers
      if (method == "zscore") {
        z <- (data[[col]] - mean(data[[col]])) / sd(data[[col]])
        # If the z-score value exceeds the threshold, consider it as an outlier
        outliers <- rbind(outliers, data[abs(z) > threshold, ])
      # Else, if method is iqr, then use the Interquartile Range (IQR) method to detect outliers
      } else if (method == "iqr") {
        Q1 <- quantile(data[[col]], 0.25)
        Q3 <- quantile(data[[col]], 0.75)
        IQR <- Q3 - Q1
        # If the value is outside the range defined by Q1 - 1.5*IQR and Q3 + 1.5*IQR, consider it as an outlier
        outliers <- rbind(outliers, data[data[[col]] < (Q1 - 1.5 * IQR) | data[[col]] > (Q3 + 1.5 * IQR), ])
      }
    }
  }
  return(outliers)
}
# Use the detect_outliers function to detect outliers in columns 1 to 18 of the Youtube_cleaned dataset
find_outliers <- detect_outliers(raw_data[numeric_vars_name])
# Print the detected outliers
print(find_outliers)
rm(find_outliers)
```
#### 3.1.5.3 Winsorize outliers
  To deal with extreme values in the dataset, we used the method of Winsorizing (shrinking the tails), which sets values greater than the 95th percentile in the dataset to the 95th percentile value. This method has unique advantages over other methods of dealing with outliers, such as truncation or deletion. 
  First, it preserves much of the original information of the data. Second, truncation reduces the potential adverse effects of extreme values or outliers on the model, thus improving the stability and predictive accuracy of the model. 
  In addition, some studies have also shown that the results of statistical hypothesis validation and model fit of the model can be effectively improved by appropriate outlier treatment [@Dixon1960].
  
First,identify which variables are numerical or categorical.并且设置好Category列的变量和Numeric列的变量

```{r, Winsorize Outliers}
# Function to truncate values in specified columns that exceed a given percentile.
truncate_tail <- function(data, column_names, percentile = 95) {
  
  # For each column, replace values above the specified percentile with the value at that percentile.
  for (col in column_names) {
    q <- quantile(data[[col]], probs = percentile / 100, na.rm = TRUE)
    data[complete.cases(data) & data[[col]] > q, col] <- q
  }
  
  return(data)
}

# Apply the function to the first 18 columns of Youtube_cleaned.
raw_data <- truncate_tail(raw_data, numeric_vars_name, percentile = 95)

```

Secondly,summarize how much outliers in each variable
对每一个列做总结有多少个离异值
```{r}
# 计算z分数并检测离群值的函数
detect_outliers_zscore <- function(data, column_names, threshold = 3) {
  outliers_count <- sapply(column_names, function(col_name) {
    z_scores <- (data[[col_name]] - mean(data[[col_name]], na.rm = TRUE)) / sd(data[[col_name]], na.rm = TRUE)
    sum(abs(z_scores) > threshold, na.rm = TRUE)
  })
  return(outliers_count)
}

# 使用缩尾处理后的数据检测离群值
outliers_count_after_truncate <- detect_outliers_zscore(raw_data, numeric_vars_name)
print(outliers_count_after_truncate)

```
Finally,we can find that the influence of outliers is slight deduced, but there are still existing some outliers, if we change all the outliers, it will over-changed and affect the modeling.
从结果发现缺失值确实有所减缓，但是我们没有办法完全处理所有的缺失值，这也会过度修改数据。到此数据已经清洁完成并且准备开始Classification

### 3.1.6 Change data type 
  Change data type from character to factor for 'category', 'country', 'channel type'
  Change some categorical variable to factors to prepare for Classification
将部分文本列调整为factor，为Classification做准备
```{r Factor type}
factor_columns <- c("category", "country","country_abbr", "channel_type")
#Use mutate adjust factor_columns, change then to factor type
raw_data <- raw_data %>%
  mutate(across(all_of(factor_columns), as.factor))
rm(factor_columns)
```


### 3.1.7 Add new column which might be used in the future.
3.6.1 Add full time column
添加时间完整的列
```{r full time}
# First, ensure that the 'created_date' column is in integer format
raw_data$created_date <- as.integer(as.character(raw_data$created_date))

# Define a vector of month names
month_names <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Create a vector of two-digit month numbers using sprintf
month_numbers <- sprintf("%02d", 1:12)

# Create a mapping between month names and month numbers
month_mapping <- setNames(month_numbers, month_names)

# Update the 'full_date' column by combining year, month, and day
raw_data$full_date <- as.Date(paste(raw_data$created_year, 
                                   raw_data$created_month %>% match(month_names) %>% month_mapping[.],
                                   sprintf("%02d", raw_data$created_date), 
                                   sep = "-"), 
                             format = "%Y-%m-%d")
rm(month_mapping,month_names,month_numbers)
```
###3.1.8 Backup the processed data 
Backup the raw_data into another name youtube_data
```{r Backup Data}
youtube_data <- raw_data
```

## 3.2 Data Transformation
### 3.2.1 Delete some unique columns
According to the instructions of project 2.Columns that have a unique value for each row should be discarded, e.g., rank, Youtuber, etc.List all the columns that have a unique values for each row and then delete those columns.
```{r Delete Unique Colunmns}
unique_col <-  names(youtube_data) %in% c("rank","youtuber","title","video_views_rank","country_rank","channel_type_rank")
youtube_data <- youtube_data[!unique_col]
```
### 3.2.2 Handle earnings
Because our classification want to analyse the earnings variable, we need to handle all things related to earnings.
Firstly, use averaging to handle highest and lowest earnings
```{r Average Earnings}
youtube_data$monthly_earning <-( youtube_data$monthly_earning_high+youtube_data$monthly_earnings_low)/2

youtube_data$yearly_earning <-( youtube_data$yearly_earning_low+youtube_data$yearly_earning_high)/2

```
Observation found that among the overall monthly income and annual income, there are rows where the monthly income is much greater than the annual income. The reason for this problem is that some rows with the lowest monthly income have na values, which turned into average values during previous processing, resulting in this unreasonable situation.
Create a function to identify all these unreasonable rows
```{r Detect Unreasonable Rows}
detect_unusual_rows <- function(data) {
  unusual_rows <- data[data$monthly_earning > data$yearly_earning, c("monthly_earning", "yearly_earning","monthly_earnings_low","monthly_earning_high","yearly_earning_low","yearly_earning_high")]
  print(unusual_rows)
  cat("The number of unreasonable rows are", nrow(unusual_rows), "rows\n")
}

detect_unusual_rows(youtube_data)

```
It can be found that these unreasonable rows account for less than 5% of all rows.To ensure the data integrity of classification,these rows can be deleted.
```{r Delete Unreasonable Rows}
youtube_data <- youtube_data[youtube_data$monthly_earning <= youtube_data$yearly_earning, ]
```
### 3.2.3 Classification Question
In this project, we want to build model to predict which income level a youtuber belongs to.
According to the definition of low income and high income in New World Bank data. According to the latest data as of July 1, 2020, those with an annual income greater than US$4,046 are considered high-income, and others can be considered low-income groups.
问题：博主的收入水平属于什么档次
特征：订阅者数、视频观看数、上传数、创建年份、视频观看增长率并且结合年收入和月收入进行判断，还有可以加上channel type(根据type进行纵向比较)。proportion
类别：高收入  vs. 低收入
```{r Check the high income proportion}
calculate_high_earning <- function(col, threshold) {
  # 使用向量化的比较来得到逻辑向量
  high_logical <- col >= threshold
  low_logical <- col < threshold
  
  # 使用sum函数计算TRUE的数量，因为在R中TRUE被认为是1，FALSE被认为是0
  high_count <- sum(high_logical)
  low_count <- sum(low_logical)
  
  # 计算百分比
  high_earning_percentage <- high_count / length(col)
  low_earning_percentage <- low_count / length(col)
  
  # 返回一个命名列表
  return(list(high_earning_percentage = high_earning_percentage, 
              low_earning_percentage = low_earning_percentage))
}
earnings_percentages <- calculate_high_earning(youtube_data$monthly_earning, 12535)
cat("There are",earnings_percentages$high_earning_percentage * 100,"% of youtuber belongs to high income level")
```
### 3.2.4 Split Data
According to the different income levels, we need to split the youtube_data into training data and testing data
```{r}

```

```[r]
new_youtube_data <- youtube_data[c(video_views, subscribers,video_views_last_30_days)]
```









# Reference
@Online{Elgiriyewithana2023,
  author = {Nidula Elgiriyewithana},
  title = {Global YouTube Statistics 2023},
  year = {2023},
  month = {Aug},
  url = {https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023},
  note = {Accessed: yyyy-mm-dd} % replace with the actual access date
}


@Manual{Wickham2016,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Hadley Wickham},
  year = {2016},
  publisher = {Springer-Verlag New York},
  note = {ISBN 978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}

@Manual{Wickham2021,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.7},
  url = {https://dplyr.tidyverse.org},
}


@Article{Friendly2002,
  author = {Michael Friendly},
  title = {Corrgrams: Exploratory displays for correlation matrices},
  journal = {The American Statistician},
  year = {2002},
  volume = {56},
  number = {4},
  pages = {316–324},
}

@Manual{McNamara2021,
  title = {skimr: Compact and Flexible Summaries of Data},
  author = {Amelia McNamara and Hao Zhu and Eduardo Arino de la Rubia and Shannon Ellis and Julia Lowndes and Michael Quinn},
  year = {2021},
  note = {R package version 2.1.3},
  url = {https://cran.r-project.org/web/packages/skimr/},
}


@Book{Healy2018,
  author = {Kieran Healy},
  title = {Data Visualization: A Practical Introduction},
  publisher = {Princeton University Press},
  year = {2018},
}


@Article{JoanesGill1998,
  title = {Comparing measures of sample skewness and kurtosis},
  author = {D. N. Joanes and C. A. Gill},
  year = {1998},
  journal = {The Statistician},
  volume = {47},
  number = {1},
  pages = {183-189}
}





1.Introduction
2.Setup
3.Data Preparation
4.Classification
4.1 Target Variable
4.2 Feature Variables
4.3 Test and Training Sets
4.4 Null model
4.5 Single Variable Model
4.6 Model Evaluation
4.7 Naive Bayes
4.8 Logistic Regression
4.9 Comparison

5 Clustering
 - 5.1 Hierarchical Clustering
5.2 Optimal Number of Clusters
5.3 Validating Clusters
5.4 Exploring Clusters
5.4.1 Job Locations
5.4.2 Salary
5.4.3 Rating

