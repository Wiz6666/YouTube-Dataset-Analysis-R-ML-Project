---
title: "CITS4009-project2"
author: "Wiz ZHANG(SID:23210735)； Xiao ZHANG"

output:   
  html_document:
    highlight: haddock
    theme: cosmo
    code_folding: hide
---
```{r setup, include=FALSE}
#Set all code chunks to display their R code by default
knitr::opts_chunk$set(echo = TRUE)
```











# Part1 - Introduction




## 1.1 - Project Background and Purpose
  In this project, our objective is to illustrate the process of modeling by using R machine learning functions. We aim to understand and apply various stages of data science including data preparation, model building, and model evaluation. We have choosen the YouTube dataset which is shared by uni coordinator. We also use the same dataset in our Project 1. The specific contributions of each member will be separated 50-50 fairly.
  
  
  
  
## 1.2 Data Source and Characteristics
  The data set analyzed can be obtained from Kaggle platform. It comes from the "Global YouTube Statistics 2023". (https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023)
  A collection of YouTube giants, this dataset offers a perfect avenue to analyze and gain valuable insights from the luminaries of the platform. With comprehensive details on top creators' subscriber counts, video views, upload frequency, country of origin, earnings, and more, this treasure trove of information is a must-explore for aspiring content creators, data enthusiasts, and anyone intrigued by the ever-evolving online content landscape. Immerse yourself in the world of YouTube success and unlock a wealth of knowledge with this extraordinary dataset.[@Elgiriyewithana2023]








# Part2 - Setup





## 2.1 Load required R packages
  We load 'ggplot2' package [@Wickham2016] for making static graphics and 'dplyr' package [@Wickham2021] for data manipulation.
  We employ scatterplot matrices and correlation heatmaps for exploratory data analysis to understand the pairwise relationships between variables [@Friendly2002]...
  We use the skimr package to obtain more comprehensive descriptive statistics of the numeric variables in the dataset [@McNamara2021]...
```{r Library, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggthemes)
library(numform)
library(treemapify)
library(timeDate)
library(lubridate)
library(reshape2)
library(ca)
library(skimr)
library(janitor)
library(flextable)
library(shiny)
library(leaflet)
library(maps)
library(RColorBrewer)
library(scales)
library(readr)
library(forecast)
library(caret)
library(knitr)
library(ROCR)
library(ROCit)
library(nortest)
library(pROC)
library(glmnet)
library(car)
library(nnet)
library(rpart)
library(rpart.plot)
```




## 2.2 Set plot theme 
  The theme is set up to enhance the readability and interpretability of the graphs, following the best practices in data visualization [@Healy2018].
```{r Theme}
project_theme <- theme(
  panel.background = element_rect(fill = "#FFFBDC"),  # Light yellow background
  panel.grid.major = element_line(color = "#FFE4A1"), # Light orange major grid lines
  panel.grid.minor = element_blank(), # Remove minor grid lines
  plot.title = element_text(size = 18, hjust = 0.5, color = "darkblue"),  # Title color and size
  axis.title = element_text(size = 16, color = "darkblue"),  # Axis title color and size
  axis.text = element_text(size = 14, color = "black"),   # Axis text color and size
  legend.title = element_text(size = 16, color = "darkblue"), # Legend title color and size
  legend.text = element_text(size = 14, color = "black"),   # Legend text color and size
  legend.background = element_rect(fill = "#FFFBDC"),  # Legend background color
  plot.background = element_rect(fill = "#FFFBDC")   # Background color of the entire plot
)
```




## 2.3 Load the main data
  In this part, we will load the main dataset, store the path in 'Data_path'
```{r readfile, warning=FALSE}
data_path <- 'Global YouTube Statistics.csv'
raw_data <- read.csv(data_path,encoding = "UTF-8")
```




## 2.4 Dataset initial overview
  Before starting in-depth analysis and modeling, it is crucial to perform an initial overview of the dataset. By using functions such as summary, str, etc., we can gain an overall understanding of the data, including data types, data distribution, missing values and possible outliers.
    This is the process that we already done in Project 1, according to Professor Wei, we don't need to do it again so we pass here. So we shorten the content about dataset overview. Mainly is using str; summary;head command to check dataset overview, we also use a function to check dataset skewness. We also shorten or even skip the EDA part in Project 2 report.
  First, we use str command to analyze the data
```{r str,results='hide', echo=FALSE, warning=FALSE}
str(raw_data)
```

The "Global YouTube Statistics" dataset contains 995 observations and 28 variables.
  Then we would use summary command to analyze data. 
  we use skimr here, which is similar to summary command but it shows more descriptive statistics for numeric variables than summary command.
```{R Skimr Summary,results='hide', echo=TRUE, warning=FALSE}
skimr::skim(raw_data) |>
  flextable::flextable() 
```

  We can see from the standard output that 'video_views_rank' has 1 missing value, country_rank has 116, 'subscribers_for_last_30_days' has 337 the most in the whole dataset.
  The numbers of missing value of 'Gross.tertiary.education.enrollment.rate','Population', 'Unemployment.rate', 'Urban_population', 'Latitude' are the same , so it should be a systematic missing issue. So we need to pay attention to this issue in the following data cleaning.
  According to the numeric.hist, it seems most of the numeric variables are right skewnessed.The 'Unemployment.rate' does not look like a normal distribution,so as 'Longitude'.
  Skewness analysis is crucial as it helps in understanding the symmetry of the data distribution [@JoanesGill1998]. We observe a higher percentage of right-skewed columns in our dataset, which implies that most numerical variables in our dataset have a distribution that has a tail on the right side, meaning the right side of the distribution is longer or fatter than the left side. This can impact the analysis and model performance because many statistical methods assume a normal distribution of the data. Right-skewed distributions are associated with higher values and outliers on the right side of the distribution which can greatly impact the mean and variance, and subsequently, distort the interpretation and performance of the model.
  Addressing skewness is often important in regression models where the assumption of normally distributed residuals is made. Therefore, recognizing the presence of skewness can help us make informed decisions about data transformation techniques, such as logarithmic transformation or Box-Cox transformation, to attempt to normalize the data distribution before applying any modeling techniques. For example, this can be particularly relevant for variables like `subscribers`, `uploads`, and `lowest_monthly_earnings`, where skewness might impact the model’s ability to generalize well to unseen data and might lead to unreliable predictions or insights.
  
  Calculate Right skewness numbers and left skewness numbers
```{r Skewness Calculator}
calculate_skewness_percentage <- function(dataframe) {
  #initialize function variables
  right_skewed_count <- 0
  left_skewed_count <- 0
  column_names <- names(dataframe)
  #count the numbers with loops
  for (column in column_names) {
    if (is.numeric(dataframe[[column]])) {
      median_value <- median(dataframe[[column]], na.rm = TRUE)
      mean_value <- mean(dataframe[[column]], na.rm = TRUE)
      if (mean_value > median_value) {
        right_skewed_count <- right_skewed_count +1
      } else if (mean_value < median_value) {
        left_skewed_count <- left_skewed_count +1
      }
    }
  }
  #calculate percentage with the count result
  right_percentage <- round((right_skewed_count / ncol(dataframe)) * 100, 2)
  left_percentage <- round((left_skewed_count / ncol(dataframe)) * 100, 2) 
  
  return(list(right_percentage, left_percentage))  
}

# Use function to get the calculation result
results <- calculate_skewness_percentage(raw_data)
cat("\n")
cat("Percentage of right-skewed columns is:", results[[1]], "%\n")
cat("Percentage of left-skewed columns is:", results[[2]], "%\n")
rm(results)
```
  The outcomes of our skewness analysis are telling and significant, revealing that 50% of the columns in our dataset are right-skewed, in contrast to a mere 21.43% being left-skewed. This preponderance of right-skewed columns is pivotal as it unveils a tendency for the distribution of most numerical variables to be tail-heavy on the right side, a characteristic that might have consequential implications for subsequent analyses and modeling efforts.
  The presence of such skewed columns underlines the existence of numerous higher values and potential outliers on the right side of the distribution. These extreme values have the capability to significantly inflate the mean and variance of the data, potentially leading to distorted interpretations and undermining the reliability and robustness of our subsequent statistical models.
  The substantial right skewness observed in specific variables such as subscribers, uploads, and lowest_monthly_earnings could particularly impact the predictive power and generalizability of the models. Such skewness could render the models less resilient and adaptive to unseen or new data, thereby possibly yielding unreliable predictions or insights.
  Employing such transformations could help mitigate the impact of skewness and align the data more closely with the assumptions of many statistical methods, predominantly enhancing the reliability and validity of our subsequent analyses and models.

  Then we use head command to keep analyzing data.
```{r head, results='hide', echo=FALSE, warning=FALSE}
head(raw_data,n=100)
```
  Base on the standard output, we can see that subscribers shows how many people subscribe the Youtuber. The Title is the same as Youtuber, we need to figure out why it is the same but in two different columns. the Music Youtuber has 119000000 subsribers but only 0 uploads , so we need to make data cleaning later.








# Part3 - Data Cleaning and Preparation




## 3.1 Data Cleaning


### 3.1.1 Check column names and make adjustments

  First, we use names() command to check each variable name.
```{r names,results='hide'}
names(raw_data)
```

  Then , we use janitor package and rename function adjust variable names to ensure uniform formatting.
  
  janitor::clean_names() is a function in the package whose main purpose is to convert the column names of dataframes into a clean, uniform format. Here is the main logic and rules of this function:
1.All characters to lowercase
2.Spaces are converted to underscores
3.Non-alphanumeric characters are removed
4.Numbers before text moved to the end
5.Underline after keyword in R)
```{r Name Changer}
raw_data <- janitor::clean_names(raw_data)
```

  From the output result we can see most of the variable names have been adjusted properly. But some of them still need manual adjustment for better understanding and clearness.
```{r Name Changer2}
raw_data <- raw_data %>%
  rename(
    #More descriptive names
    video_views_last_30_days = video_views_for_the_last_30_days,
    country_abbr = abbreviation,
    #Shorter names
    monthly_earnings_low = lowest_monthly_earnings,
    monthly_earning_high = highest_monthly_earnings,
    yearly_earning_low = lowest_yearly_earnings,
    yearly_earning_high = highest_yearly_earnings,
    subs_last_30_days = subscribers_for_last_30_days,
    tertiary_edu_enrollment = gross_tertiary_education_enrollment,
    )
print(colnames(raw_data))
```
  
  
### 3.1.2 Inspect & Adjust data types
Use sapply function to check data types, adjust wrong data typs.
```{r Data type, results='hide', echo=FALSE,warning=FALSE}
data_types <- sapply(raw_data,class)
data_sample <- sapply(raw_data,head)
print(data_types)
print(data_sample)
rm(data_types, data_sample)
```

  From the standard output first, we know that we need to handle the missing values; 'created_month' should change from char type to numeric type this is useful for following analysis; 'category','country','channel_type','abbreviation' shoule be factor type which is better for following analysis;handle the outliers according to the skimmr result.
  
  
### 3.1.3 Handle Missing value
  We've identified three representations for missing data in our dataset: "0", "NaN", and "nan". Our goal is to standardize these by converting them to NA. Following this conversion, we aim to tally the number of NA values in each column. It's our assumption that "0" serves as a sentinel value indicating missing data. 
```{r Kill 0 &NA and delete NA}
#Change 0 or 'nan' or 'NaN' to NA value
#Char columns
raw_data[raw_data == 'nan' | raw_data == "NaN" | raw_data == 0] <- NA
#Numeric columns
raw_data <- raw_data |>
  mutate(across(where(is.numeric), ~ifelse(is.nan(.), NA, .)))
```

  Use function to calculate how many missing values in each column, handle them.
```{r col_NA calculator}
# Calculate missing value number
count_missing <- function(df) {
  sapply(df, FUN = function(col) sum(is.na(col)) )
}
# Calculate percentage of missing value
percentage_missing <- function(df) {
  sapply(df, FUN = function(col) round(sum(is.na(col)) / length(col) * 100, 2) )
}

nacounts <- count_missing(raw_data)
napercents <- percentage_missing(raw_data)

# output the result
hasNA = which(nacounts > 0)
data.frame(Column = names(nacounts[hasNA]), 
           Missing_Values = nacounts[hasNA], 
           Percentage_Missing = napercents[hasNA])
rm(col,hasNA,nacounts,napercents)

```
  Our dataset examination reveals pervasive missing data across multiple columns. Given that our dataset comprises only 995 rows, a sweeping removal of missing values could severely curtail the dataset's utility for subsequent modeling. In light of this, we've devised tailored strategies for addressing different variable types.

Before delving into the strategies for managing missing values, it's crucial to outline which variables we have decided not to process and provide the rationale behind these choices:
  1.Abbreviation: Although it's feasible to replace its missing entries with 'Unknown', a total of 122 missing entries renders it impractical for further processing.
  2.Latitude & Longitude: These columns lack intrinsic geographical data. With missing data exceeding 10% of the total, they are deemed not fit for subsequent modeling tasks.
  3.Rank, Subscribers, Title & Youtuber: These columns will not be part of our processing pipeline since they manifest no missing values.

Now, let's delve into the specifics of our chosen strategies for different columns:
  1.Video Views: With a scant presence of missing values, we grappled with options between row deletion or median imputation. Given our preference to retain as many rows as possible and the data's skewed distribution, we've opted for median imputation.
  2.Country & Abbreviation: The magnitude of missing values (122 in total) poses challenges. Hence, rather than impute, we'll earmark these columns for exclusion from subsequent analyses.
  3.Earnings: Given its skewed distribution, our chosen strategy leans towards median imputation.
  4.Latitude & Longitude: These will be excluded from further analysis due to substantial missing values and an absence of auxiliary geographical information to assist imputation.
  5.Time Variables: These aren't continuous time series, ruling out techniques like ARIMA modeling. Given the paucity of missing data (only 5 rows), which equates to a mere 0.5% of the dataset, we've resolved to delete rows with missing time entries, without attempting imputation.
          
The numeric columns slated for processing are as follows:
  c("video_views", "uploads", "video_views_rank", "country_rank", "channel_type_rank", "video_views_last_30_days", "monthly_earnings_low", "monthly_earning_high", "yearly_earning_low", "yearly_earning_high", "subs_last_30_days", "tertiary_edu_enrollment", "population", "unemployment_rate", "urban_population").

The categorical columns in our crosshairs are:
  c("category", "country", "country_abbr", "channel_type").

  Our immediate focus will be on the aforementioned numeric columns. Leaning on the outcomes of our skewness analysis, which indicated a predominantly right-skewed data distribution, we've favored median imputation over mean imputation. This is predicated on the fact that the median, unlike the mean, won't perturb the right-skewed nature of the data.

In summary, our overarching strategy centers on the preservation of data integrity, and our chosen imputation methods are reflective of each column's inherent properties and data distribution.
```{r Handle missing value (numeric cols)}
change_to_median_cols <- c("video_views",
"uploads","video_views_rank","country_rank","channel_type_rank","video_views_last_30_days","monthly_earnings_low",	"monthly_earning_high","yearly_earning_low",	 "yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate","urban_population")
for (col in change_to_median_cols) {
    median_val <- median(raw_data[[col]], na.rm = TRUE)
    raw_data[[col]][is.na(raw_data[[col]])] <- median_val
}
rm(col,median_val,change_to_median_cols)
```

  Change the NAs in character variables into "Unknown"
```{r Handle missing value (character cols)}
character_columns <- c("category", "country","country_abbr","channel_type")
  
for (col in character_columns) {
  raw_data[[col]][is.na(raw_data[[col]])] <- "Unknown"
}
rm(col,character_columns)
```

  Delete all the missing rows (5 rows) in the time column. Since there are only 5 rows, it does not affect the subsequent analysis. At the same time, delete the rows with the channel creation year less than 2005, because the YouTube company was created in 2005.
```{r Delete rows}
raw_data <- raw_data %>%
  filter(!is.na(created_year) & !is.na(created_month) & !is.na(created_date) & created_year >= 2005)
```

  Overall, only 6 rows of data were deleted, which does not affect the subsequent analysis because it only accounts for 0.6% of the data.


### 3.1.4 Handle repeating lines
```{r Inpsect repeat lines}
# Use unique function handle repeating lines
raw_data_unique <- unique(raw_data)
raw_data <- raw_data_unique
rm(raw_data_unique)
```


### 3.1.5 Handle outliers

#### 3.1.5.1 classify variables
  Change the variables into numerical and categorical according to the data type.
```{r classify variables}
numeric_vars <- raw_data[sapply(raw_data,is.numeric)]
categorical_vars <- raw_data[sapply(raw_data,function(x) class(x) %in% c('factor','character'))]
numeric_vars_name <- names(numeric_vars)
categorical_vars_name <- names(categorical_vars)
```

#### 3.1.5.2 Identify outliers
  Create a function to identify outliers for all variables in the dataset
```{r Outliers detection}
# Define a function to detect outliers in the dataset
detect_outliers <- function(data, method = "zscore", threshold = 3) {
  # Initialize an empty dataframe to store detected outliers
  outliers <- data.frame()
  # Loop over each column of the dataset
  for (col in colnames(data)) {
    # Check if the column is numeric as outliers can be detected only in numeric columns
    if (is.numeric(data[[col]])) {
      # If method is zscore, then use the z-score method to detect outliers
      if (method == "zscore") {
        z <- (data[[col]] - mean(data[[col]])) / sd(data[[col]])
        # If the z-score value exceeds the threshold, consider it as an outlier
        outliers <- rbind(outliers, data[abs(z) > threshold, ])
      # Else, if method is iqr, then use the Interquartile Range (IQR) method to detect outliers
      } else if (method == "iqr") {
        Q1 <- quantile(data[[col]], 0.25)
        Q3 <- quantile(data[[col]], 0.75)
        IQR <- Q3 - Q1
        # If the value is outside the range defined by Q1 - 1.5*IQR and Q3 + 1.5*IQR, consider it as an outlier
        outliers <- rbind(outliers, data[data[[col]] < (Q1 - 1.5 * IQR) | data[[col]] > (Q3 + 1.5 * IQR), ])
      }
    }
  }
  return(outliers)
}
# Use the detect_outliers function to detect outliers in columns 1 to 18 of the Youtube_cleaned dataset
find_outliers <- detect_outliers(raw_data[numeric_vars_name])
# Print the detected outliers
print(find_outliers)
rm(find_outliers)
```

#### 3.1.5.3 Winsorize outliers
  To deal with extreme values in the dataset, we used the method of Winsorizing (shrinking the tails), which sets values greater than the 95th percentile in the dataset to the 95th percentile value. This method has unique advantages over other methods of dealing with outliers, such as truncation or deletion. 
  First, it preserves much of the original information of the data. Second, truncation reduces the potential adverse effects of extreme values or outliers on the model, thus improving the stability and predictive accuracy of the model. 
  In addition, some studies have also shown that the results of statistical hypothesis validation and model fit of the model can be effectively improved by appropriate outlier treatment [@Dixon1960].
  
First,identify which variables are numerical or categorical.并且设置好Category列的变量和Numeric列的变量

```{r Winsorize outliers}
# Function to truncate values in specified columns that exceed a given percentile.
truncate_tail <- function(data, column_names, percentile = 95) {
  
  # For each column, replace values above the specified percentile with the value at that percentile.
  for (col in column_names) {
    q <- quantile(data[[col]], probs = percentile / 100, na.rm = TRUE)
    data[complete.cases(data) & data[[col]] > q, col] <- q
  }
  
  return(data)
}

# Apply the function to the first 18 columns of Youtube_cleaned.
raw_data <- truncate_tail(raw_data, numeric_vars_name, percentile = 95)

```

  Secondly,summarize how much outliers in each variable
```{r Outliers Count}
# Function to compute z-scores and detect outliers based on a specified threshold
detect_outliers_zscore <- function(data, column_names, threshold = 3) {
  outliers_count <- sapply(column_names, function(col_name) {
    # Calculate z-scores for each specified column
    z_scores <- (data[[col_name]] - mean(data[[col_name]], na.rm = TRUE)) / sd(data[[col_name]], na.rm = TRUE)
    # Count the number of absolute z-scores that exceed the provided threshold
    sum(abs(z_scores) > threshold, na.rm = TRUE)
  })
  return(outliers_count)
}

# Detect outliers in the data after truncation using the function above
outliers_count_after_truncate <- detect_outliers_zscore(raw_data, numeric_vars_name)
print(outliers_count_after_truncate)
rm(outliers_count_after_truncate,numeric_vars,categorical_vars)
```
  Ultimately, we observe a reduction in the impact of outliers, but it's impractical to address all of them. Overcorrecting could potentially distort the data. With the cleaning phase now complete, we're ready to transition to the classification stage.


### 3.1.6 Change data type 
  Change data type from character to factor for 'category', 'country', 'channel type'
  Change some categorical variable to factors to prepare for Classification
```{r Factor type}
factor_columns <- c("category", "country","country_abbr", "channel_type")
#Use mutate adjust factor_columns, change then to factor type
raw_data <- raw_data %>%
  mutate(across(all_of(factor_columns), as.factor))
rm(factor_columns)
```


###3.1.7 Backup the processed data 
Backup the raw_data into another name youtube_data
```{r Backup Data}
youtube_data <- raw_data
clustering_data <- raw_data 
```



### 3.1.8 Add new columns
  Add new columns which might be used in the future.

#### 3.1.8.1 Full time column
```{r full time}
# First, ensure that the 'created_date' column is in integer format
# Assuming the data doesn't contain any decimal parts or NA values
youtube_data$created_date <- as.integer(as.character(raw_data$created_date))

# Check for any non-integer values in 'created_date'
if (any(!is.finite(youtube_data$created_date))) {
  stop("The 'created_date' column contains non-integer or NA values.")
}

# Define a vector of month names
month_names <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Create a vector of two-digit month numbers using sprintf
month_numbers <- sprintf("%02d", 1:12)

# Create a mapping between month names and month numbers
month_mapping <- setNames(month_numbers, month_names)

# Update the 'full_date' column by combining year, month, and day
youtube_data$full_time <- as.Date(paste(raw_data$created_year, 
                                   raw_data$created_month %>% match(month_names) %>% month_mapping[.],
                                   sprintf("%02d", youtube_data$created_date), 
                                   sep = "-"), 
                             format = "%Y-%m-%d")
rm(month_mapping, month_names, month_numbers)
```

#### 3.1.8.2 Subscriber engagement column
  video_views/subscribers. This can give an idea of how many videos, on average, each subscriber watches, offering a measure of engagement for the channel.
```{r Subscriber engagement}
youtube_data$subs_engagement <- youtube_data$video_views/youtube_data$subscribers
```

#### 3.1.8.3 Add video upload frequency column
  uploads/(2023-created_year). This can show how many videos a YouTuber uploads on average each year since they created their channel.
```{r video upload frequency}
youtube_data$uploads_frequencey <- youtube_data$uploads/(2023-youtube_data$created_year)
```

#### 3.1.8.4 Average views per video column
  video_views/uploads. This can help in understanding the average number of views per video, offering a measure of the content's popularity.
```{r Average views per video}
youtube_data$average_views_per_video <- youtube_data$video_views/youtube_data$uploads
```

#### 3.1.8.5 Urbanization rate column
  urban_population/population. This will give a percentage of urbanization in a country.
```{r  Urbanization rate}
youtube_data$urban_rate <- youtube_data$urban_population/youtube_data$population
```

#### 3.1.8.6 Years of experience column
  2022 - created_year. The number of years since the channel was created, which might relate to income and popularity.
```{r Years of experience}
youtube_data$age <- 2022-youtube_data$created_year
```

#### 3.1.8.7 Education population ratio column
  tertiary_edu_enrollment/population. This gives a ratio of the population that's undergoing higher education in each country
```{r Education population ratio}
youtube_data$edu_ratio <- youtube_data$tertiary_edu_enrollment/youtube_data$population
```

#### 3.1.8.8 Top channel flag column
  A binary feature based on rank (if channel rank < 1000 then 1, else 0).
```{r Top channel flag}
youtube_data$top_channel_flag <- as.numeric(youtube_data$channel_type_rank <1000)
```

#### 3.1.8.9 Video views increasing rate column
  video_views_last_30_days/ video_view. This shows the increasing rate about video views.
```{r video_views_increasing_rate}
youtube_data$video_views_increasing_rate <- (youtube_data$video_views_last_30_days/youtube_data$video_views)
```

#### 3.1.8.3 Add average earning columns
  Average earning for each month and each year.
```{r Average Earnings}
youtube_data$monthly_earning <-( youtube_data$monthly_earning_high+youtube_data$monthly_earnings_low)/2
youtube_data$yearly_earning <-( youtube_data$yearly_earning_low+youtube_data$yearly_earning_high)/2
```

  Observation found that among the overall monthly income and annual income, there are rows where the monthly income is much greater than the annual income. The reason for this problem is that some rows with the lowest monthly income have na values, which turned into average values during previous processing, resulting in this unreasonable situation.
  Create a function to identify all these unreasonable rows
```{r Detect unreasonable earnings rows}
detect_unusual_rows <- function(data) {
  unusual_rows <- data[data$monthly_earning > data$yearly_earning, c("monthly_earning", "yearly_earning","monthly_earnings_low","monthly_earning_high","yearly_earning_low","yearly_earning_high")]
  print(unusual_rows)
  cat("The number of unreasonable rows are", nrow(unusual_rows), "rows\n")
}
detect_unusual_rows(youtube_data)
```
  It can be found that these unreasonable rows account for less than 5% of all rows.To ensure the data integrity of classification,these rows can be deleted. According to the discussion with Sirui Liu in Teams, it is acceptable to delete rows less than 5% in total. For now we delete 45 rows(4.6%).
```{r Delete Unreasonable Rows}
youtube_data <- youtube_data[youtube_data$monthly_earning <= youtube_data$yearly_earning, ]
```




## 3.2 Data Preparation


### 3.2.1 Object and features
In this project, we would build model and predict the high earning Youtuber on the top list.
For now we pick some features like:
  "subscribers","video_views","uploads","channel_type","created_year",
  "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate"
Predict variable：high/low earning
All the features and category variables have been prepared in 3.1.8 Section.

Data Integration：
  We would use the World Bank GDP data from the Project2 prject description, which includes GDP per capita for each country for each year. Based on the country field merge the GDP for 2022 year. We decided to use 2022 year GDP for each country as a reference. Because our Youtuber data is for 2023, 2022 is the closest year to 2023 in the GDP data, which ensures that real-word economic fluctuations will have the least impact because the years are closest.
  After manually check, we realized that the first 4 rows of the GDP csv file are not data columns, so we skipped the first 4 rows using the skip parameter. When loading, our GPD data would only load up to the 2005 GDP and could not load the rest right side columns. So we solved the problem by using ChatGPT4 to learn how to select specific columns using the readr package.
```{r gdp data path, echo=FALSE, message=FALSE, warning=FALSE}
gdp_data_path <- "API_NY.GDP.PCAP.CD_DS2_en_csv_v2_5871588.csv"

gdp_data <- read_csv(gdp_data_path, skip = 4)%>%
  select(-c("Country Code", "Indicator Name", "Indicator Code","...68"))

colnames(gdp_data)[colnames(gdp_data) == "Country Name"] <- "Country.Name"

gdp_data <- as.data.frame(gdp_data)

gdp_data <- gdp_data %>%
  mutate(gdp_per_capita = `2022`)
```
Before merge there are some Country.Names that different from the Country.Name in youtube dataset.Change them to the same countries
```{r}
(diff_countries = setdiff(youtube_data$country, gdp_data$Country.Name))
gdp_data$Country.Name[gdp_data$Country.Name == "Egypt, Arab Rep."] <- "Egypt"
gdp_data$Country.Name[gdp_data$Country.Name == "Korea, Rep."] <- "South Korea"
gdp_data$Country.Name[gdp_data$Country.Name == "Russian Federation"] <- "Russia"
gdp_data$Country.Name[gdp_data$Country.Name == "Turkiye"] <- "Turkey"
gdp_data$Country.Name[gdp_data$Country.Name == "Venezuela, RB"] <- "Venezuela"
```
change all the GDP NA into that year of average GDP value
```{r}
replace_na_with_global_median <- function(data) {
  years <- colnames(data)[-1]  # 获取年份列名
  global_median_gdp <- numeric(length(years))
  
    # 将 GDP 列转换为数值数据类型
  for (i in 2:ncol(data)) {
    data[, i] <- as.numeric(data[, i])
  }
  
  # 计算每年的全球中位数GDP
  for (i in 1:length(years)) {
    global_median_gdp[i] <- median(data[!is.na(data[, years[i]]), years[i]], na.rm = TRUE)
  }
  
  # 遍历每一年的每一个国家的GDP值，使用 apply 函数
  data[, years] <- t(apply(data[, years], 1, function(row) {
    is_na <- is.na(row)
    row[is_na] <- global_median_gdp[is_na]
    return(row)
  }))
  

  
  return(data)
}

# 使用函数替代NA值
gdp_data <- replace_na_with_global_median(gdp_data)


```

 Calculate GDP value based on Youtube channel age
```{r}
# Divide the per capital GDP value in those years of channel age by the channel age in these years
calculate_channel_age_adjusted_income <- function(youtube_data, gdp_data) {
  # 初始化一个空的向量来存储结果
  adjusted_income <- numeric(nrow(youtube_data))
  
  # 遍历 youtube_data 的每一行
  for (i in 1:nrow(youtube_data)) {
    # 获取当前行的国家和创建年份
    country <- as.character(youtube_data$country[i])
    created_year <- as.numeric(as.character(youtube_data$created_year[i]))  # 将年份转换为数字
    
    # 在 gdp_data 中查找相应的国家行
    gdp_row <- gdp_data[gdp_data$`Country.Name` == country, ]
    created_year_column <- which(names(gdp_row) == created_year)
    
    if (nrow(gdp_row) == 0) {
      # 如果没有找到匹配的国家数据，则将结果设为 NA
      adjusted_income[i] <- NA
    } else {
      # 找到匹配的国家数据后，根据创建年份找到对应年份范围的 GDP 数据
      gdp_values <- as.numeric(gdp_row[ , (created_year_column + 1):64])
      
      # 计算数值1
      value1 <- sum(gdp_values, na.rm = TRUE)
      
      # 计算数值2
      value2 <- 1 / youtube_data$age[i]
      
      # 计算最终结果
      adjusted_income[i] <- youtube_data$yearly_earning[i] / value2
    }
  }
  
  # 将结果存储到 youtube_data 的 channel_age_adjusted_income 列中
  youtube_data$channel_age_adjusted_income <- adjusted_income
  
  return(youtube_data)
}

# 使用函数计算和更新 adjusted income
youtube_data <- calculate_channel_age_adjusted_income(youtube_data, gdp_data)

```
handle NAs in the channel_age_adjusted_income column
```{r}
# 计算中位数
median_value <- median(youtube_data$channel_age_adjusted_income, na.rm = TRUE)

# 遍历 channel_age_adjusted_income 列
for (i in 1:length(youtube_data$channel_age_adjusted_income)) {
  if (is.na(youtube_data$channel_age_adjusted_income[i])) {
    youtube_data$channel_age_adjusted_income[i] <- median_value
  }
}
```

Use merge command here.
```{r merge gdp data}
gdp_2022 <- gdp_data[,c("Country.Name","gdp_per_capita")]

youtube_data <- merge(youtube_data,gdp_2022, by.x = "country",by.y = "Country.Name", all.x = TRUE)
rm(gdp_2022)
```

  If the Country.Name is "Unknown", then we would use the medium of youtube_data$gdp_per_capita to swap it, it won't affect our data too much, and also, this is the same handling method that we used before in 3.1.3 Handle Missing value. Consistency is ensured here.
```{r handle unknown country}
median_gdp <- median(youtube_data$gdp_per_capita,na.rm = TRUE)
youtube_data$gdp_per_capita[is.na(youtube_data$gdp_per_capita)] <- median_gdp
rm(median_gdp)
```




Normalize Youtuber earning： 
  We start by using Youtuber yearly earning divided by GDP per capita to observe how many time the Youtuber's income is the country's per capita income. Here we use annual income divided by GPD per capita, the reason for using average annual income rather than average monthly income is that anuual income is more stable and reduces fluctuations in income due to seasonality or other short term factors. Secondly, annual income is more objective when compared to annual GDP per capita.
```{r normalized earning}
youtube_data$normalized_earning <- as.numeric(youtube_data$yearly_earning/youtube_data$gdp_per_capita)
```


Logic for defining high income:
    We would like to use the mean and add a certain number of standard deviations as a threshold for high income based on the standard deviation, but with this approach we have to first assume that income is normally distributed, which is not always the case in reality.
    
    
Validation Definition:
  We want to verify that our data is normally distributed by visualizing a graph.
```{r check normalized earning}
summary(youtube_data$normalized_earning)
```
As we can see there is a huge gap between the median and the mean. So we continue with kernel density estimates, and statistical tests, here we used the Shapiro-Wilk test; the Kolmogorov-Smirnov test; and the Anderson-Darling test
```{r normalization test}
#Kernel Density Estimation
plot(density(youtube_data$normalized_earning), main="Kernel Density Estimation of Normalized Earnings", xlab="Normalized Earnings")

# Shapiro-Wilk test
shapiro_test <- shapiro.test(youtube_data$normalized_earning)
print(shapiro_test)
# Kolmogorov-Smirnov test
ks_test <- ks.test(youtube_data$normalized_earning, "pnorm", mean(youtube_data$normalized_earning), sd(youtube_data$normalized_earning))
print(ks_test)
# Anderson-Darling test
ad_test <- ad.test(youtube_data$normalized_earning)
print(ad_test)
```
Shapiro-Wilk normality test.
  The value of W is 0.44508.The value of this statistic ranges from 0 to 1, where 1 indicates that the data perfectly fits a normal distribution. However, the resulting W value is much less than 1, which is an initial indication that the data may not be normally distributed. Moreover, the p-value is very close to 0 (less than 2.2e-16), which means that we should reject the original hypothesis of normality.
Kolmogorov-Smirnov test.
  The D-value is 0.34033. this is the largest gap between the Earning data distribution and the perfect normal distribution. Again, the p-value is very small (less than 2.2e-16) which further confirms that the data is not normally distributed. The test also gives a warning about the presence of duplicate values (ties) in the data. This is because we used the median instead of the Youtuber for the unknown country.
Anderson-Darling normality test.
  The value of A is 191.44. the larger this value, the more the data deviates from the normal distribution. As with the previous two tests, a very small p-value (less than 2.2e-16) indicates that the data are not normally distributed.
Summary: 
  All three tests consistently show that the youtube_data$normalized_earning data does not currently conform to a normal distribution.


Adjustment and Optimization:
  While modern machine learning algorithms do not require the data to meet the assumption of a normal distribution. However, bringing the data closer to a normal distribution can improve model performance, especially for models such as linear regression and logistic regression.
  We observe some very large values from the density plot, they are much larger than thousands of times the average GDP, and for these values they are simply too different. We will use a logarithmic transformation.The results of data analysis show that after Log-transformation, the data presents different distribution characteristics[Feng2014]. More conducive to classification, which is what we learned from class to reduce the effect of these too large values. After that the multiples were adjusted to between 0-10, and according to the Density image, it's also leaning more towards a normal distribution.
```{r log transformation}
youtube_data$normalized_earning <- log(youtube_data$normalized_earning + 1)  
```

```{r Check normalized earning}
summary(youtube_data$normalized_earning)
# Shapiro-Wilk test
shapiro_test <- shapiro.test(youtube_data$normalized_earning)
print(shapiro_test)

# Kolmogorov-Smirnov test
ks_test <- ks.test(youtube_data$normalized_earning, "pnorm", mean(youtube_data$normalized_earning), sd(youtube_data$normalized_earning))
print(ks_test)

# Anderson-Darling test
ad_test <- ad.test(youtube_data$normalized_earning)
print(ad_test)
# Density plot
plot(density(youtube_data$normalized_earning), main="Kernel Density Estimation of Normalized Earnings", xlab="Normalized Earnings")

rm(ad_test,ks_test,shapiro_test)
```

Shapiro-Wilk Test.
  The w-value is 0.98524, which is close to 1. This is usually a good sign that the data is close to a normal distribution. However, the p-value = 3.358e-08 (very small) is much less than 0.05
Kolmogorov-Smirnov Test.
  D-value is 0.038866 which is small. p-value = 0.1134 which is greater than 0.05
Anderson-Darling Test.
  A value of 2.2485, large value. p-value = 1.056e-05 (very small), much less than 0.05

  youtube_data$normalized_earning is close to a normal distribution, although it is not exactly normal. Many real-world applications have data that do not strictly follow a normal distribution, but if they are close enough, many of the properties of a normal distribution are still useful.
  When defining high income based on mean and standard deviation, we choose the mean plus one times the standard deviation: about 84% of the data points will be less than this value. Thus, this would be a relatively loose definition of high income that includes about 16% of YouTubers.
```{r}
earning_threshold <- median(youtube_data$normalized_earning)+sd(youtube_data$normalized_earning)
print(earning_threshold)
```
It means if a Youtuber's yearly earning is 6.2 times larger than GDP per capita then this Youtuber is a high earning Youtuber.

### 3.2.2 Tranform categrical variables
Because the first time we built the decision tree model, the model had too many nodes, resulting in overfitting. This is the second time to re-establish the model, so we need to modify the categorical variables and change the categorical variables into factors with fewer levels. This may enhance the generalization ability of the model.
First, Check the number of occurrences of each country
```{r}
print(table(youtube_data$country))
```
Turn country variables into factors and then divide them into developed countries, developing countries, and unknown levels.
```{r}
youtube_data$country_status <- factor(ifelse(youtube_data$country %in% c("Japan","Germany","Canada","United States", "United Kingdom","Denmark", "Australia","China","South Korea", "Switzerland", "Sweden", "Finland", "France", "Norway", "Singapore", "Netherlands", "Italy", "Brazil", "Belgium", "Greece", "Iceland", "Luxembourg", "Spain"), "Developed", ifelse(youtube_data$country == "Unknown", "Unknown", "Developing")))
```
Second, There are too many levels of channel types, which may also lead to overfitting, so the channel occurrence number is less than or equal to 20 and unknown are classified into the others category.
```{r}
print(table(youtube_data$channel_type))
print(table(youtube_data$channel_type) < 20)
```
```{r}
youtube_data$channel_type_status <- factor(ifelse(youtube_data$channel_type %in% c("Animals","Autos","Nonprofit","Sports","Tech","Unknown"),"Others",as.character(youtube_data$channel_type)))
```

### 3.2.3 Remove unrelevant columns
  According to the instructions of project 2. Columns that have a unique value for each row should be discarded, e.g., rank, Youtuber, etc.List all the columns that have a unique values for each row and then delete those columns.
```{r Remove unrelevant colunmns}
youtube_data <- youtube_data[
  c(
    "normalized_earning",
    "subscribers","video_views","uploads","channel_type_status","created_year","category",
    "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate","channel_age_adjusted_income","country_status"
    )
  ]
```


### 3.2.4 Feature Engineering
We have already created required features in 3.1.8 Section.
  "subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate"
  Here, we would use 6.2 times as threshold, build a new binary column, if Youtuber earning is larger than 6.2 return 1, else 0.
```{r}
youtube_data$earning_class <- ifelse(youtube_data$normalized_earning > 6.2, 1, 0)
youtube_data <- subset(youtube_data, select = -normalized_earning)
```


```{r}
names(youtube_data)
```


### 3.2.5 Training and Testing dataset 
  According to the different income levels, we need to split the youtube_data into training data and testing data. We do a 20/80 split to form the training and test sets.
  After google many articles and learn caret pacakge by ourselves, we decide to use "creatDataPartition" function in caret package, it ensures that the distribution of the target variable is taken into account when partitioning the data.[@Kuhn2019]
```{r}
# Set a seed for reproducibility
set.seed(562816)

# Split the data into training and testing sets using a stratified sampling method. 
train_index <- createDataPartition(youtube_data$earning_class, p = 0.8, list = FALSE, times = 1)

# Subset the original dataset to create training and testing datasets based on the indices generated above.
youtube_train <- youtube_data[train_index, ]
youtube_test  <- youtube_data[-train_index, ]

# Identify and store the names of all independent variables (i.e., all variables excluding the target 'earning_class').
independent_variables <- setdiff(colnames(youtube_data), "earning_class")

# From the list of independent variables, identify and store the names of numeric and integer type variables.
numeric_independent_variables <- independent_variables[sapply(youtube_data[, independent_variables], class) %in% c("numeric", "integer")]

# Similarly, from the list of independent variables, identify and store the names of factor and character type variables.
categorical_independent_variables <- independent_variables[sapply(youtube_data[, independent_variables], class) %in% c("factor", "character")]
```








# Part4 Classification




## 4.1 Dependent Variable & Independent Variable
Dependent Variable: earning_class
Independent varibale: "subscribers","video_views","uploads","channel_type_status","created_year","category","subs_engagement","uploads_frequencey","average_views_per_video"     "urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate","earning_class","channel_age_adjusted_income","country_status"   




## 4.2 Feature Selection

### 4.2.1 Function build up
  Build single variable model function. Use Single variable model for each variable to determine which has the most predictive power.
```{r utility-functions}
#Define target column and positive label
target_column <- 'earning_class'
positive_label <- '1'
```


```{r single variable model function}
Single_variable_model <- function(target_column, feature_column, prediction_column){
  
  # Ensure that the input vectors are of the same length
  if (!(length(target_column) == length(feature_column) && length(target_column) == length(prediction_column))) {
    stop("All input vectors must have the same length")
  }

  # Small constant to avoid division by zero
  epsilon <- 1.0e-3
  
  # Calculate the overall probability of the positive class
  probility_of_positive <- sum(target_column == positive_label) / length(target_column)
  
  # For NAs in feature_column, compute the distribution of target_column
  na_table <- table(as.factor(target_column[is.na(feature_column)]))
  
  # For NAs in feature_column, compute the conditional probability of the positive class
  probility_of_positive_with_na <- ifelse(is.na((na_table / sum(na_table))[positive_label]), probility_of_positive, (na_table / sum(na_table))[positive_label])
  
  # Create a table to represent the relationship between different values of feature_column and target_column
  value_table <- table(as.factor(target_column), feature_column, useNA = "ifany")
  
  # Compute conditional probabilities for each value of feature_column
  probility_of_positive_with_val <- (value_table[positive_label, ] + epsilon * probility_of_positive) / (colSums(value_table) + epsilon)
  
  # Generate predictions based on conditional probabilities
  predictions <- probility_of_positive_with_val[prediction_column]
  
  # Handle NA values in prediction_column
  predictions[is.na(prediction_column)] <- probility_of_positive_with_na
  
  # Handle values in prediction_column that didn't appear in the training data
  predictions[is.na(predictions)] <- probility_of_positive
  
  # Return predictions
  return(predictions)
}

```

  Build up AUC calculator function.
  
```{r AUC calculator function}
AUC_calculator <- function(predcol, outcol) {
    perf <- performance(prediction(predcol, outcol == positive_label), 'auc')
    return(as.numeric(perf@y.values))
}
```

```{r discretize numeric variables}
# 将数值型列离散化的函数
discretizeVariable <- function(column) {
    quantiles <- quantile(column, probs = seq(0, 1, 0.1), na.rm = TRUE)
    discrete_column <- cut(column, unique(quantiles))
    return(discrete_column)
}
```


```{r single variable model function for discretized numeric variables}
# 单一变量预测的函数（使用离散化的数值特征）
SingleVariablePredictNumeric <- function(target_column, feature_column, prediction_column, subset) {
    feature_discrete <- discretizeVariable(feature_column[subset])
    prediction_discrete <- discretizeVariable(prediction_column[subset])
    return(Single_variable_model(target_column[subset], feature_discrete, prediction_discrete))
}

```


### 4.2.2 Analyze Variables

#### 4.2.2.1 Analyzing Categorical variables:

  Call Single variable model & AUC calculator function， check AUC value for categorical independent variables.
```{r analyse categorical variables}

for (var in categorical_independent_variables) {
    prediction_probs <- Single_variable_model(target_column = youtube_train$earning_class, 
                                             feature_column = youtube_train[, var],
                                             prediction_column = youtube_train[, var])
    auc <- AUC_calculator(prediction_probs, youtube_train$earning_class)
    if (auc >= 0.1) {
        print(sprintf("%s - AUC: %4.3f", var, auc))
    }
}
```


#### 4.2.2.2 Analyzing Numeric variables:
  Check the AUC value for numeric independent variables. We need to change the numeric data into category data first as the instruction in Week9 Slides.
```{r analyse numeric variables}
# 对每个数值型特征进行处理
for (var in numeric_independent_variables) {
    pred_col_name <- paste('pred', var, sep = '_')
    youtube_train[, pred_col_name] <- SingleVariablePredictNumeric(youtube_train$earning_class, youtube_train[, var], youtube_train[, var])
    auc_train <- AUC_calculator(youtube_train[, pred_col_name], youtube_train$earning_class)
    if (auc_train >= 0.55) {
        print(sprintf("%s: AUC: %4.3f", var, auc_train))
    }
}

```

开始使用100次交叉重复验证
100次重复交叉验证
```{r}
# 单一变量预测的函数（使用离散化的数值特征）

# 对每个数值型特征进行处理
vars <- numeric_independent_variables 

for (var in vars) {
    aucs <- rep(0, 100)
    
    for (rep in 1:length(aucs)) {
        useForCalRep <- rbinom(n=nrow(youtube_train), size=1, prob=0.1) > 0
        
        predRep <- SingleVariablePredictNumeric(youtube_train$earning_class, 
                                                youtube_train[, var], 
                                                youtube_train[, var], 
                                                !useForCalRep)  # Pass the subset to the function
        
        # Only calculate AUC for the subset used for prediction
        actual_subset_labels = youtube_train$earning_class[!useForCalRep]
        aucs[rep] <- AUC_calculator(predRep, actual_subset_labels)
    }
    
    print(sprintf("%s: mean: %4.3f; sd: %4.3f", var, mean(aucs), sd(aucs)))
}

```
urban_rate (0.892) 和 edu_ratio (0.886) 
依然是最强的预测特征。这两个特征在预测目标变量上的表现非常好。

video_views_increasing_rate (0.779) 和 uploads_frequencey (0.745) 
仍然是较强的特征，预测能力强。

video_views (0.682), subs_engagement (0.680) 和 average_views_per_video (0.672)
AUC值仍然相对较高，表现为中等的预测能力。
    
uploads (0.716) 
AUC值稍有增加，但仍在前面提到的范围内。

subscribers (0.623), created_year (0.598) 和 age (0.592)
AUC值略有改变，但相对较低。它们仍然比随机预测更好，但预测能力较弱。

top_channel_flag (0.551) 
AUC值最低，这表明它可能对预测目标变量的贡献较小。




### 4.2.3 ROC plot


建立ROC函数 ， 用shiny输出ROC图表，
```{r}
library(ROCit)

plot_roc <- function(predcol, outcol, colour_id=2, overlaid=FALSE) {
    ROCit_obj <- rocit(score = predcol, class = outcol == positive_label)
    par(new = overlaid)
    plot(ROCit_obj, col = c(colour_id, 1), legend = FALSE, YIndex = FALSE, values = FALSE)
}
```


1.使用Shiny展示ROC plot考虑到读者多样性，我们同时展示两个ROC图表以便读者需要对比不同的ROC
```{r}
# Load Shiny app script
#source("app.R")

# Run Shiny app
#shinyApp(ui= ui_double_roc_plot, server = server_double_roc_plot)

```






double density comparing AUC
```{r double density plot,fig.width=18}
library(ggplot2)
library(gridExtra)

# 假设我们想比较前两个分类变量的AUC（确保它们的AUC都大于0.8）
var1 <- categorical_independent_variables[1]
var2 <- categorical_independent_variables[2]

prediction_probs1 <- Single_variable_model(target_column = youtube_train$earning_class, 
                                          feature_column = youtube_train[, var1],
                                          prediction_column = youtube_train[, var1])

prediction_probs2 <- Single_variable_model(target_column = youtube_train$earning_class, 
                                          feature_column = youtube_train[, var2],
                                          prediction_column = youtube_train[, var2])

fig1 <- ggplot(youtube_train) + geom_density(aes(x = prediction_probs1, color = as.factor(earning_class)))
fig2 <- ggplot(youtube_train) + geom_density(aes(x = prediction_probs2, color = as.factor(earning_class)))

grid.arrange(fig1, fig2, ncol=2)
```



## 4.3 Null model
### 4.3.1 Build Null Model
By analyzing only the target variable, a null model is created and the AUC of the model is evaluated. The AUC value of the null model can help comparing single variable.
```{r build null model}
# Calculate the number of positive class instances (Npos)
Npos <- sum(youtube_train[, target_column] == 1)
cat("Number of positive class (target_column == 1) in youtube_train:", Npos, "rows\n")

# Calculate the number of negative class instances (Nneg)
Nneg <- nrow(youtube_train) - Npos
cat("Number of negative class (target_column == 0) in youtube_train:", Nneg, "rows\n")

# Calculate the Null Model prediction, which is the proportion of positive class
pred.Null <- Npos / (Npos + Nneg)
cat("Proportion of target_column == 1 in youtube_train (Null Model prediction):", pred.Null, "\n")

# Create a vector of Null Model predictions with the same length as the dataset
null_model_predictions <- rep(pred.Null, nrow(youtube_train))

# Calculate the Null Model AUC using the pROC package
library(pROC) 
roc_obj <- roc(youtube_train[, target_column], null_model_predictions)
null_model_auc <- auc(roc_obj)

cat("Null Model AUC:", null_model_auc, "\n")

```
## 4.4 Select suitable single variable
### 4.4.1 Likelihood and deviance
According to the knowledge of lectures, If the AUC value of a single variable model is close to or lower than the AUC value of the null model, it means that the single variable model may not provide enough information for predicting the target variable. In this case, using this single variable model may not have a significant impact on model performance.
使用likelihood去比较单变量和空变量
```{r}
positive_label <- '1'
# Define a function to compute log likelihood 
logLikelihood <- function(ytrue, ypred, epsilon=1e-6) {
                          sum(ifelse(ytrue==positive_label, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T)
}
# Compute the likelihood of the Null model on the Training model
logNull <- logLikelihood(youtube_train[,target_column], sum(youtube_train[,target_column]==positive_label)/nrow(youtube_train))
cat(logNull)
```
#### 4.4.1.1 Deviance of Categorical Variables
```{r}
# By adjusting the value of minDrop, we can control how strictly the variables are selected
selCatVars <- c()
minDrop <- 10 #可能还需要调整值的大小
for (v in categorical_independent_variables) {
    pi <- paste( 'pred',v, sep='')
    binary_var <- ifelse(youtube_train$v == "desired_level", 1, 0)
    devDrop <- 2 * (logLikelihood(youtube_train[, target_column], binary_var) - logNull)
    if (devDrop >= minDrop) {
      print(sprintf("%s, deviance reduction: %g", pi, devDrop))
      selCatVars <- c(selCatVars, pi)
 }
}

```
The deviance values of the two categorical variables are larger than the null model, indicating that these two features are suitable for model prediction under this method.
#### 4.3.2.2 Deviance of Numeric Variables
```{r warning=FALSE}
selNumVars <- c()
minDrop <- 0 # 可能还需要调整值的大小
for (v in numeric_independent_variables) {
  pi <- paste(v, sep='')
  epsilon <- 1e-6  # Small constant to avoid division by zero
  devDrop <- 2 * (logLikelihood(youtube_train[, target_column], youtube_train[, pi], epsilon) - logNull)
  if (devDrop >= minDrop) {
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    selNumVars <- c(selNumVars, pi)
 }
}
```
Set the value of minDrop to 0 and traverse all numerical variables. There are originally 12 numerical variables, but only 9 have results. This means that in this selection method, only the deviance values of these 9 variables are better than the null model. Large, you can choose to do model predictions.
### 4.4.2 Chi-Square Test
```{r warning=FALSE}
analyze_features <- function(data, target_column, categorical_features, numeric_features) {
  results <- list()
  
  # 分析分类特征
  for (feature in categorical_features) {
    cross_table <- table(data[, feature], data[, target_column])
    chi_square <- chisq.test(cross_table)
    
    result <- list(
      Feature = feature,
      Analysis = "Chi-Square Test",
      P_Value = chi_square$p.value
    )
    
    results <- append(results, result)
  }
  
  # 分析数值特征
  for (feature in numeric_features) {
    anova_result <- aov(data[, target_column] ~ data[, feature])
    
    result <- list(
      Feature = feature,
      Analysis = "ANOVA",
      P_Value = summary(anova_result)[[1]][["Pr(>F)"]]
    )
    
    results <- append(results, result)
  }
  
  return(results)
}

# 使用函数分析特征
results <- analyze_features(youtube_data, target_column, categorical_independent_variables, numeric_independent_variables)

# 打印分析结果
for (result in results) {
  print(result)
}
summary(results)
```

### 4.4.2 Using Logistic Regression
Using only one method for feature selection may have limitations, so use a combination of two methods to select one or more features that are most suitable for model prediction.
Reasons for choosing to use logistic regression: Logistic regression is a linear model, and the magnitude and direction of the model parameters (coefficients) can provide an intuitive understanding of the impact of features on the target variable. This helps explain the model's predictions. And by observing the coefficients of logistic regression, the importance of each feature to the target variable can be estimated. Features with larger absolute values of coefficients have a more significant impact on the target variable. And logistic regression can automatically select the most relevant features, and the coefficients of irrelevant features can be shrunk to zero by adjusting the regularization term (such as L1 regularization). This helps reduce dimensionality and reduce model complexity.[Hastie2009]
The independent variable is binary which 1 represent high income level and 0 represents low income level. We need to choose the binary logistic regression.
```{r warning=FALSE}
selVars <- c("channel_type_status","category","subscribers","video_views","uploads","created_year","subs_engagement","uploads_frequencey","average_views_per_video","urban_rate","age","edu_ratio","top_channel_flag","video_views_increasing_rate","channel_age_adjusted_income","country_status" )
deviance_formula <- paste(target_column, '>0 ~ ', paste(selVars, collapse=' + '), sep='')
cat(deviance_formula)
cat("Feature dimension = ", length(selVars))
gmodel <- glm(as.formula(deviance_formula), data=youtube_train, family=binomial(link='logit'))
summary(gmodel)
```
According to the result of the gmodel, the smaller the p value, the stronger the correlation between the feature and the dependent variable.We can find the result of 'age' feature is NA, which has the possibility of a strong correlation with other features. This situation is more complex to analyze, so we decided not to use this feature.
Combining the analysis of likelihood and logistic regression, we finally selected the features that are strongly correlated with the independent variable earning_class: subscribers, subscribers_engagement, and urban_rate to build prediction model.
## 4.5 Descision Tree Classifier
### 4.5.1 Build the model performacne measure function and plotting ROC function
#### 4.5.1.1 Calculate the Model's accuracy, precision, recall, and f1 score
Use different evaluation methods to evaluate the representation of the decision tree model
```{r}
# ytrue should be a vector containing 1s (or TRUE) and 0s (or FALSE);
# ypred should be a vector containing the predicted probability values for the target class.
# Both ytrue and ypred should have the same length.
performanceMeasures <- function(ytrue, ypred, model.name = "model", threshold=0.5) {
      # compute the normalised deviance
      dev.norm <- -2 * logLikelihood(ytrue, ypred)/length(ypred)
      # compute the confusion matrix
      confmatrix <- table(actual = ytrue, predicted = ypred >= threshold)
      accuracy <- sum(diag(confmatrix)) / sum(confmatrix)
      precision <- confmatrix[2, 2] / sum(confmatrix[, 2])
      recall <- confmatrix[2, 2] / sum(confmatrix[2, ])
      f1 <- 2 * precision * recall / (precision + recall)
      data.frame(model = model.name, precision = precision, recall = recall, f1 = f1, dev.norm = dev.norm)
}

# pander formating
panderOpt <- function(){
    library(pander)
    # setting up Pander Options
    panderOptions("plain.ascii", TRUE)
    panderOptions("keep.trailing.zeros", TRUE)
    panderOptions("table.style", "simple")
}

# Prettier Performance Table Function
pretty_perf_table <- function(model, xtrain, ytrain,
xtest, ytest, threshold=0.5) {
    # Option setting for Pander
    panderOpt()
    perf_justify <- "lrrrr"
    
    # call the predict() function to do the predictions
    pred_train <- predict(model, newdata=xtrain)
    pred_test <- predict(model, newdata=xtest)
    
    # comparing performance on training vs. test
    trainperf_df <- performanceMeasures(
    ytrain, pred_train, model.name="training", threshold=threshold)
    testperf_df <- performanceMeasures(
    ytest, pred_test, model.name="test", threshold=threshold)
    
    # combine the two performance data frames using rbind()
    perftable <- rbind(trainperf_df, testperf_df)
    pandoc.table(perftable, justify = perf_justify)
}
```
#### 4.5.1.2 Plot the ROC
build a function to generate the ROC plot
```{r}
library(ROCit)
plot_roc <- function(predcol1, outcol1, predcol2, outcol2){
    roc_1 <- rocit(score=predcol1, class=outcol1==positive_label)
    roc_2 <- rocit(score=predcol2, class=outcol2==positive_label)
    plot(roc_1, col = c("blue","green"), lwd = 3,
    legend = FALSE,YIndex = FALSE, values = TRUE, asp=1)
    lines(roc_2$TPR ~ roc_2$FPR, lwd = 3,
    col = c("red","green"), asp=1)
    legend("bottomright", col = c("blue","red", "green"),
    c("Test Data", "Training Data", "Null Model"), lwd = 2)
}
```

### 4.5.2 Build Descision Tree Model
#### 4.5.2.1 Build Descision Tree Model for all features
首先需要建立完整的决策树模型，然后根据这个完整的决策树模型进行剪枝，根据剪枝前后的性能变化，决定使用哪些节点进行剪枝以获得更好的泛化性能。
##### a. Build the model
```{r build decision tree model,fig.width=15,fig.height=20}
formula_Var1 <- paste(target_column,'> 0 ~ ',paste(c(categorical_independent_variables,numeric_independent_variables), collapse=' + '), sep='')
tree_model_all <- rpart(formula_Var1, data=youtube_train)

summary(tree_model_all)

# 可视化决策树
rpart.plot(tree_model_all)

```
##### b. Calculate this Model's AUC Scores
```{r}
tmodel_all_train_auc <- AUC_calculator(predict(tree_model_all, newdata=youtube_train), youtube_train[,target_column])
cat('This model\'s AUC score on the training set is',tmodel_all_train_auc, "\n")

#test set AUC score
tmodel_all_test_auc <- AUC_calculator(predict(tree_model_all, newdata=youtube_test), youtube_test[,target_column])
cat('This model\'s AUC score on the testing set is',tmodel_all_test_auc)
```
##### c. Print the performance table
分析第一个决策树模型
```{r}
pretty_perf_table(tree_model_all,youtube_train[c(categorical_independent_variables,numeric_independent_variables)],youtube_train[,target_column] == positive_label,youtube_test[c(categorical_independent_variables,numeric_independent_variables)],youtube_test[,target_column] == positive_label)
```
##### d. Print the ROC plot
```{r}
pred_test_roc_tmodel_all <- predict(tree_model_all, newdata=youtube_test)
pred_train_roc_tmodel_all <- predict(tree_model_all, newdata=youtube_train)
plot_roc(pred_test_roc_tmodel_all, youtube_test[[target_column]],
         pred_train_roc_tmodel_all, youtube_train[[target_column]])
```
#### 4.5.2.2 Build Descision Tree Model for all numerical features
##### a. Build the model
这个图像有点过度拟合，有可能的原因是feature里面的categorical variable里面的层次太多，导致过度拟合，尝试只使用feature里面所有的numerical variable新建model。
```{r}
library(rpart)
library(rpart.plot)
(formula_Var1 <- paste(target_column,'> 0 ~ ', paste(c(numeric_independent_variables), collapse=' + '), sep=''))
tree_model_all_num <- rpart(formula_Var1, data=youtube_train)

summary(tree_model_all_num)

# 可视化决策树
rpart.plot(tree_model_all_num)
```
##### b. Calculate this Model's AUC Scores
先算一下AUC得分
```{r}
tmodel_all_num_train_auc <- AUC_calculator(predict(tree_model_all_num, newdata=youtube_train), youtube_train[,target_column])
cat('This model\'s AUC score on the training set is',tmodel_all_num_train_auc, "\n")

#test set AUC score
tmodel_all_num_test_auc <- AUC_calculator(predict(tree_model_all_num, newdata=youtube_test), youtube_test[,target_column])
cat('This model\'s AUC score on the testing set is',tmodel_all_num_test_auc)
```

##### c. Print the performance table
```{r}
pretty_perf_table(tree_model_all_num,youtube_train[c(numeric_independent_variables)],youtube_train[,target_column] == positive_label,youtube_test[c(numeric_independent_variables)],youtube_test[,target_column] == positive_label)
```
##### d. Print the ROC plot
```{r}
pred_test_roc_tmodel_all_num <- predict(tree_model_all_num, newdata=youtube_test)
pred_train_roc_tmodel_all_num <- predict(tree_model_all_num, newdata=youtube_train)
plot_roc(pred_test_roc_tmodel_all_num, youtube_test[[target_column]],
         pred_train_roc_tmodel_all_num, youtube_train[[target_column]])
```

#### 4.5.2.3 Build Descision Tree Model for predition features
##### a. Build the model
```{r build decision tree model,fig.width=15,fig.height=8}
formula_Var2 <- paste(target_column,'> 0 ~ ', paste(c('subscribers', 'subs_engagement','urban_rate'), collapse=' + '), sep='')
tree_model_pre <- rpart(formula_Var2, data=youtube_train)

summary(tree_model_pre)

# 可视化决策树
rpart.plot(tree_model_pre)

```
##### b. Calculate this Model's AUC Scores
```{r}
# train set AUC score 
tmodel_pre_train_auc <- AUC_calculator(predict(tree_model_pre, newdata=youtube_train), youtube_train[,target_column])
cat('This model\'s AUC score on the training set is',tmodel_pre_train_auc, "\n")

#test set AUC score
tmodel_pre_test_auc <- AUC_calculator(predict(tree_model_pre, newdata=youtube_test), youtube_test[,target_column])
cat('This model\'s AUC score on the testing set is',tmodel_pre_test_auc)
```
##### c. Print the performance table
```{r}
pretty_perf_table(tree_model_pre,youtube_train[c('subscribers', 'subs_engagement','urban_rate')],youtube_train[,target_column] == positive_label,youtube_test[c('subscribers', 'subs_engagement','urban_rate')],youtube_test[,target_column] == positive_label)
```
##### d. Print the ROC plot
```{r}
pred_test_roc_tmodel_pre <- predict(tree_model_pre, newdata=youtube_test)
pred_train_roc_tmodel_pre <- predict(tree_model_pre, newdata=youtube_train)
plot_roc(pred_test_roc_tmodel_pre, youtube_test[[target_column]],
         pred_train_roc_tmodel_pre, youtube_train[[target_column]])
```


## 4.6 Naïve Bayes Classifier
### 4.6.1 Build the model
直接用三个预测feature来建模
```{r}
nb_model <- naiveBayes(earning_class ~ subscribers + subs_engagement + urban_rate, data = youtube_train)
summary(nb_model)
```
### 4.6.2 Doing prediction
```{r}
predictions_nb <- predict(nb_model, newdata = youtube_train)
```
### 4.6.3 Confusion Matrix
```{r}
confusion_matrix_nb <- table(predictions_nb, youtube_train$earning_class)
```
### 4.6.4 AUC value
```{r}
nbmodel_pred_train_auc <- AUC_calculator(predict(nb_model, newdata=youtube_train), youtube_train[,target_column])
cat('This model\'s AUC score on the training set is',nbmodel_pred_train_auc, "\n")

#test set AUC score
nbmodel_pred_test_auc <- AUC_calculator(predict(nb_model, newdata=youtube_test), youtube_test[,target_column])
cat('This model\'s AUC score on the testing set is',nbmodel_pred_test_auc)
```
### 4.6.5 print the performance table
```{r}
pretty_perf_table(nb_model,youtube_train[c('subscribers', 'subs_engagement','urban_rate')],youtube_train[,target_column] == positive_label,youtube_test[c('subscribers', 'subs_engagement','urban_rate')],youtube_test[,target_column] == positive_label)
```
### 4.6.6 Plot the ROC
```{r}
pred_test_roc_nbmodel <- predict(nb_model, newdata=youtube_test)
pred_train_roc_nbmodel <- predict(nb_model, newdata=youtube_train)
plot_roc(pred_test_roc_nbmodel, youtube_test[[target_column]],
         pred_train_roc_nbmodel, youtube_train[[target_column]])
```
# Part 5 Cluster
### 5.1 Backup the processed data 
Backup the raw_data into another name youtube_data
```{r Backup Data}
youtube_data <- raw_data
clustering_data <- raw_data 
```




















# 5. Clustering
首先，你需要
准备或获取已经标准化的数据（如scaled_df）。
使用上述函数计算不同k值下的CH指数，并确定最佳的k值。
使用最佳的k值进行层次聚类。
绘制并查看聚类的树状图


数据预处理
## 导入数据
此处 我们使用 预处理后的整个清理数据集

```{r}
clustering_data <- clustering_data[,c("country","category","rank","subscribers","video_views",
"uploads","video_views_last_30_days","monthly_earnings_low","monthly_earning_high","yearly_earning_low","yearly_earning_high","subs_last_30_days","tertiary_edu_enrollment","population","unemployment_rate" ,"urban_population")]
```

更换国家为World，更换种类为Others

```{r}
# 计算每个国家出现的次数
country_counts <- table(clustering_data$country)
categories_counts <- table(clustering_data$category)

# 更换国家为World，更换种类为Others
countries_to_replace <- names(country_counts[country_counts < 5])
categories_to_replace <- names(categories_counts[categories_counts < 5])

# 替换这些国家名称为"World"
clustering_data$country[clustering_data$country %in% countries_to_replace] <- "World"
clustering_data$country[clustering_data$country == "Unknown"] <- "World"

clustering_data$category[clustering_data$category %in% categories_to_replace] <- "Others"

# 重新定义因子级别
clustering_data$country <- as.factor(clustering_data$country)
clustering_data$category <- as.factor(clustering_data$category)

# 按国家汇总数据并计算每个国家的平均值
clustering_data_avg <- clustering_data %>% 
  group_by(country) %>%
  summarize(across(where(is.numeric), ~mean(.)))

# 标准化
clustering_data_avg[sapply(clustering_data_avg, is.numeric)] <- scale(clustering_data_avg[sapply(clustering_data_avg, is.numeric)])

# 移除非数值列
clustering_numeric_data <- clustering_data[, sapply(clustering_data, is.numeric)]
```


1. 数据汇总：首先按国家进行分组，然后对每个国家的数值变量求平均。这意味着数据集小得多，因为只包含每个国家的平均值。我尝试过不使用数据汇总，这会使决策树图表有极多的末端，导致下端全黑。

2. 数据标准化：选择国家的平均值标准化









```{r}
# 导入所需的库
library(ggplot2)
library(fpc)

# ...[数据导入和预处理的代码部分保持不变]...

# 距离方法和连接方法组合
distance_methods <- c("manhattan", "euclidean")
linkage_methods <- c("average", "single", "ward.D2")

# 使用列表存储所有聚类结果
all_cluster_results <- list()

# 嵌套循环，对每种组合进行层次聚类
for (dist_method in distance_methods) {
  for (linkage_method in linkage_methods) {
    # 计算距离矩阵
    dist_matrix <- dist(clustering_numeric_data, method = dist_method)
    # 进行层次聚类
    cluster_result <- hclust(dist_matrix, method = linkage_method)
    # 保存到列表中
    all_cluster_results[[paste(dist_method, linkage_method, sep="_")]] <- cluster_result
  }
}

```




```{r}
# 导入必要的库
library(ggplot2)
library(fpc)

print_clusters <- function(df, groups, cols_to_print) {
  Ngroups <- max(groups)
  for (i in 1:Ngroups) {
    print(paste("cluster", i))
    print(df[groups == i, cols_to_print])
  }
}
# 距离方法和连接方法的组合
distance_methods <- c("manhattan", "euclidean")
linkage_methods <- c("average", "single", "ward.D2")

# 用于存储所有聚类结果的列表
all_cluster_results <- list()

# 使用嵌套循环为每种组合进行层次聚类
for (dist_method in distance_methods) {
  for (linkage_method in linkage_methods) {
    cat(paste("使用距离方法:", dist_method, "和连接方法:", linkage_method, "\n"))
    
    # 计算距离矩阵
    dist_matrix <- dist(clustering_numeric_data, method = dist_method)
    
    # 进行层次聚类
    cluster_result <- hclust(dist_matrix, method = linkage_method)
    
    # 结果保存到列表中
    all_cluster_results[[paste(dist_method, linkage_method, sep="_")]] <- cluster_result
    
    # 绘制树状图
    plot(cluster_result, labels = FALSE, main = paste("距离方法:", dist_method, ", 连接方法:", linkage_method))
    
    # 选择聚类数量并提取每个聚类的成员
    groups <- cutree(cluster_result, k = 3)
    print_clusters(clustering_data, groups, c("country"))
  }
}

```















```{r}
# 提取每个聚类的成员
groups <- cutree(cluster_result, k=3)

# 打印每个聚类的成员
print_clusters <- function(df, groups, cols_to_print) {
  Ngroups <- max(groups)
  for (i in 1:Ngroups) {
    print(paste("cluster", i))
    print(df[groups == i, cols_to_print])
  }
}

cols_to_print <- c("country")
print_clusters(clustering_data, groups, cols_to_print)

# 可视化聚类 - 数据准备
princ <- prcomp(clustering_numeric_data)
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=clustering_numeric_data)[,1:nComp])
hclust_project2D <- cbind(project2D, cluster=as.factor(groups), country=clustering_data$country)

# 可视化聚类 - 寻找凸包
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,
          lapply(unique(groups),
                 FUN = function(c) {
                   f <- subset(proj2Ddf, cluster==c)
                   f[chull(f),]
                 }
          )
  )
}

hclust_hull <- find_convex_hull(hclust_project2D, groups)

# 使用ggplot可视化聚类
library(ggplot2)
ggplot(hclust_project2D, aes(x=PC1, y=PC2)) +
  geom_point(aes(shape=cluster, color=cluster)) +
  geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) +
  geom_polygon(data=hclust_hull, aes(group=cluster, fill=as.factor(cluster)), alpha=0.4, linetype=0) +
  theme(text=element_text(size=20))

```





```{r}
# 导入fpc库，这个库提供了clusterboot()函数
library(fpc)

# 设置最佳的聚类数目
kbest.p <- 5

# 使用clusterboot()函数对clustering_numeric_data进行层次聚类
# 使用hclustCBI作为聚类方法，使用ward.D2作为连接方法，并选择kbest.p作为最佳的聚类数目
cboot.hclust <- clusterboot(clustering_numeric_data, clustermethod=hclustCBI,
                            method="ward.D2", k=kbest.p)

# 显示clusterboot的结果概览
summary(cboot.hclust$result)

# 从clusterboot的结果中获取数据点的聚类标签
groups.cboot <- cboot.hclust$result$partition

# 使用之前定义的print_clusters()函数展示每个聚类中的国家
cols_to_print <- c("country")
print_clusters(clustering_data, groups.cboot, cols_to_print)

# 计算每个聚类的稳定性值，这是基于解散次数的比例
# 1 - (每个聚类解散的次数 / 100)
stability_values <- 1 - cboot.hclust$bootbrd/100

# 识别并显示两个最稳定的聚类
cat("因此，聚类", order(stability_values)[kbest.p-1], "和", order(stability_values)[kbest.p-2], "是非常稳定的")


```

```{r}
# 从聚类结果中选择聚类5的国家
cluster_5_countries <- clustering_data[groups.cboot == 5, "country"]
print("聚类5的国家:")
print(cluster_5_countries)

# 从聚类结果中选择聚类1的国家
cluster_1_countries <- clustering_data[groups.cboot == 1, "country"]
print("聚类1的国家:")
print(cluster_1_countries)

```






references.bib
title: 
output: html_document
bibliography: references.bib

# Reference
@Online{Elgiriyewithana2023,
  author = {Nidula Elgiriyewithana},
  title = {Global YouTube Statistics 2023},
  year = {2023},
  month = {Aug},
  url = {https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023},
  note = {Accessed: yyyy-mm-dd} % replace with the actual access date
}


@Manual{Wickham2016,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Hadley Wickham},
  year = {2016},
  publisher = {Springer-Verlag New York},
  note = {ISBN 978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}

@Manual{Wickham2021,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.7},
  url = {https://dplyr.tidyverse.org},
}


@Article{Friendly2002,
  author = {Michael Friendly},
  title = {Corrgrams: Exploratory displays for correlation matrices},
  journal = {The American Statistician},
  year = {2002},
  volume = {56},
  number = {4},
  pages = {316–324},
}

@Manual{McNamara2021,
  title = {skimr: Compact and Flexible Summaries of Data},
  author = {Amelia McNamara and Hao Zhu and Eduardo Arino de la Rubia and Shannon Ellis and Julia Lowndes and Michael Quinn},
  year = {2021},
  note = {R package version 2.1.3},
  url = {https://cran.r-project.org/web/packages/skimr/},
}


@Book{Healy2018,
  author = {Kieran Healy},
  title = {Data Visualization: A Practical Introduction},
  publisher = {Princeton University Press},
  year = {2018},
}


@Article{JoanesGill1998,
  title = {Comparing measures of sample skewness and kurtosis},
  author = {D. N. Joanes and C. A. Gill},
  year = {1998},
  journal = {The Statistician},
  volume = {47},
  number = {1},
  pages = {183-189}
}

@online{Kuhn2019,
  author    = {Max Kuhn},
  title     = {The caret Package 4.1 Simple Splitting Based on the Outcome},
  year      = {2019},
  url       = {http://topepo.github.io/caret/data-splitting.html},
  note      = {Accessed: 2023-10-11}
}

@Article{Feng2014,
author = {Feng, C. and Wang, H. and Lu, N. and Chen, T. and He, H. and Lu, Y. and Tu, X. M.},
title = {Log-transformation and its implications for data analysis},
journal = {Shanghai Arch Psychiatry},
year = {2014},
volume = {26},
number = {2},
pages = {105-109},
}

@Book{Hastie2009,
  author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  publisher = {Springer},
  year = {2009},
}


1.Introduction
2.Setup
3.Data Preparation
4.Classification
4.1 Target Variable
4.2 Feature Variables
4.3 Test and Training Sets
4.4 Null model
4.5 Single Variable Model
4.6 Model Evaluation
4.7 Naive Bayes
4.8 Logistic Regression
4.9 Comparison

5 Clustering
 - 5.1 Hierarchical Clustering
5.2 Optimal Number of Clusters
5.3 Validating Clusters
5.4 Exploring Clusters
5.4.1 Job Locations
5.4.2 Salary
5.4.3 Rating

